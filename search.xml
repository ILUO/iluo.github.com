<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MLDS18(2) Optimization]]></title>
    <url>%2F2018%2F10%2F20%2FMLDS18-2-Optimization%2F</url>
    <content type="text"><![CDATA[这是李宏毅老师2018年春季深度学习课程的第二章，主要讲了神经网络的损失函数是非凸的，但是我们仍然有办法去达到local minima，但是如何达到global minima，还有待探索。 优化(Optimization)：是否可能在网络可以表示的函数空间中找到使训练误差的函数。在深度学习中，损失函数是一个非凸函数，非凸函数的优化问题是一个NP难问题。 神经网络的损失函数是非凸的结合下图，考虑一个只有一层隐藏层的全连接的网络，如果通过优化得到的参数能够使损失函数降到一个最小值，现在将隐藏层的神经元进行错位排列，并将对应的参数错位置换，可以想象得出这个网络所能表示的函数空间并没有发生变化（因为结构没变），而且相关参数随着神经元的错排进行了错位置换后，对应的损失函数的值也没有变。但是在以参数为应变量，损失函数为自变量的空间中，可以发现：将参数进行错位后，它对应的最小值的位置与原位置是不一样的，考虑有n个参数，则可以发现有很多个点都可以达到这个最小值，所以深度学习的损失函数是非凸的。 Hessian Matrixcritical pointcritical(待鉴定的，临界的) point(接下来称为cp)，cp的梯度是为0，对于非凸函数，梯度为0不代表处于极值，还有可能是鞍点(saddle point)，即该点的梯度为0，但是在某些方向有上升的趋势，在某些方向有下降的趋势(想象一下马鞍的中心)，而不是像极值点一样，周围的变化趋势的是一致的，所以在深度学习中，对于cp，要借助Hessian Matrix进行判断。 Hessian在之前的一篇博客中，描述了梯度下降的数学原理，即通过一阶泰勒展开来寻求梯度为0的点，因为之前是针对梯度为0则一定处于极值的凸函数进行优化，所以一阶展开就行了。但是对于非凸函数，一阶泰勒展开近似时梯度为0并不能代表该点是极值，所以要进一步近似，寻求更微小的变化趋势，能够表示的就是Hessian矩阵。 f(\theta) = f(\theta^0)+(\theta-\theta^0)^Tg+\frac{1}{2}(\theta-\theta^0)^TH(\theta-\theta^0)+...上式的g是梯度，是一个向量：$g_i = \frac{\delta f(\theta^0)}{\delta \theta_i}$H就是Hessian矩阵而且是对称的:$H_{ij} = \frac{\delta^2}{\delta\theta_i\delta\theta_jf(\theta^0)}=H_{ji}$ 而当梯度g为0时，有$f(\theta) = f(\theta^0)+\frac{1}{2}(\theta-\theta^0)^TH(\theta-\theta^0)+…$的近似，所以就要考虑Hessian矩阵是如何影响这个近似的。 通过牛顿法求二阶展开的critical point对二阶近似两边进行微分取0，可以得到(没有验证)： \Delta f(\theta) \approx g + H(\theta - \theta_0) = 0\theta = \theta^0 - H^{-1}g所以参数的更新就如下图，比梯度下降看起来步骤会更少，但是在高维空间中计算$H^{-1}$的代价会非常大，而且仍然不能避免让结果靠近鞍点。 分析Hessian梯度g为0时，处于critical potint，有： f(\theta) = f(\theta^0)+\frac{1}{2}(\theta-\theta^0)^TH(\theta-\theta^0)+...将$\theta-\theta_0$看作一个向量x，如果矩阵H是正定的，那么$x^THx &gt; 0$，即H的所有特征值都是正的，那么在$\theta_0$附近的函数值都大于$f(\theta_0)$，所以这个点是local minima。如果是负定的，那么$x^THx &lt; 0$，同理，这个点是local maxima，H的特征值都是负的。如果$x^THx$有些大于0，有些小于0，即特征值有正有负，那么这个点是鞍点。但是如果H矩阵半正定或者半负定，即特征值有0的存在，那么就无法判断这个点的具体情况了，因为可能会被未考虑的近似所影响。 证明设$v$为一个单元特征向量，由于$v^THv=v^T(\lambda v)=\lambda||v||^2=\lambda$，根据线代知识，H是一个对称方阵，它的所有特征向量都是正交的，而且正好能够张成$\theta$所在的空间，即$\theta_0$的到$\theta$的变化方向可以被这些特征向量的线性组合所表示，那么就有$(\theta-\theta_0)=m_1v_1+m_2v_2+…+m_nv_n$，$v_1$到$v_n$是所有的特征向量，通过这样表示，代入二阶泰勒展开，可以发现$f(\theta)$与$f(\theta_0)$的关系，完全由特征值的值所决定。 Deep Linear NetWork考虑如图最简单的Linear Network的Hessian矩阵，可以发现梯度为0时，即便在鞍点也可以通过计算Hessian来判断往哪一个方向走。 考虑如图有两个hidden layers的Linear Network，有三种情况可以造成critical point，第一种情况，$W_1W_2W_3=1$，观察损失函数可以判断这个点是global minima，对于第二种情况,Hessian 矩阵是个0阵，无法通过Hessian矩阵来判断。对于第三种情况，H的特征值有正有负，所以可以确定是鞍点。 对于更加复杂的网络，还可以通过从不同的两个维度取点来观察函数的变化图像，可以观察出来是不是鞍点。 一些关于Linear Network的minima的论文一些结论：隐藏层的大小大于输入和输出的维度时是global minima超过两层隐藏层的网络可能会制造出没有负特征值的鞍点。 Kenji Kawaguchi, Deep Learning without Poor Local Minima, NIPS, 2016 Haihao Lu, Kenji Kawaguchi, Depth Creates No Bad Local Minima, arXiv,2017 Thomas Laurent, James von Brecht, Deep linear neural networks with arbitrary loss: All local minima are global, arXiv, 2017 Maher Nouiehed, Meisam Razaviyayn, Learning Deep Models: Critical Points and Local Openness, arXiv, 2018 Non-linear Deep NetWorkReLU has localReLU的network是有盲点的，由于ReLU的特性，会出现输入数据x后所有输出都是0，那么这个点的梯度就是0，因为在数据点x附近都是非常平坦的，所以对于这个critical point，只要存在另外的输入使y为负数（这很好得到），所以这个点就不是global minima。 关于Non-linear Deep Network的假想：使用某些特殊的参数初始化，或者特殊的训练数据，可以找到global minima 关于deep learning的猜想猜想一：几乎所有的局部最小值和全局最小值都是差不多的。如果Hessian矩阵的特征值是大于0还是小于0的概率各为1/2，那么随着参数的增多，如果遇到cp，是鞍点的概率会越来越大。而特征值符号的概率p其实是由损失函数决定的，如下图，鞍点容易出现在loss高的地方，而localminima会出现在loss比较低的地方，从而有local minima都是差不多的猜想。 猜想二：如果Hessian的正特征值越多，就越像一个local minima,如下图。 猜想三：如果网络够大，可以通过梯度下降找到一个全局最优解。 Razvan Pascanu, Yann N. Dauphin, Surya Ganguli, Yoshua Bengio, On the saddle point problem for non-convex optimization, arXiv, 2014 Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, Yoshua Bengio, “Identifying and attacking the saddle point problem in high-dimensional non-convex optimization”, NIPS, 2014 Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, Yann LeCun, “The Loss Surfaces of Multilayer Networks”, PMLR, 2015 Jeffrey Pennington, Yasaman Bahri, “Geometry of Neural Network Loss Surfaces via Random Matrix Theory”, PMLR, 2017Benjamin D. Haeffele, Rene Vidal, ” Global Optimality in Neural Network Training”, CVPR, 2017 关于损失函数的可视化观察参数是如何影响损失函数的一个维度：选择初始化的点与训练结束的点相连，然后继续延长两倍，然后可视化这一段线段上loss在某一个方向上的变化。 两个维度：固定某一个参数方向的变化，另一个参数方向则是在不断的变化的。 可视化训练过程在每次update参数的时候记录下所有的参数以及对应的loss，最后将参数通过PCA降维到二维，最后将对应的loss可视化到二维平面上，如下图，不同的模型可能会得到不同的Solution。 比较不同的优化算法使用同样的数据源喂给不同算法得到的网络，比较他们的差异。为什么会有这种差异（在鞍点时可能选择了不同的路线） 作业做了前两个作业，收获最大的就是理解了tensorflow的优化器是分为两个步骤的，第一个步骤是compute_gradients，即计算梯度，第二个步骤是apply_gradients，即使用计算得到的梯度和上一次的参数去更新此次参数。具体代码放在了github上 Visualize the Optimization Process即上面的可视化训练过程，记录每次的参数和损失函数，然后使用PCA降维参数，将权重展示到平面上，我使用神经网络模拟同一个函数训练了六次，六次的变化如下，和老师的看起来差不多吧(迫真)： Observe Gradient Norm During Training即记录每次训练的Gradient Norm和loss，然后放到一起比较，是在MNist_data数据集上跑的，如图： 所以有Gradient Norm接近0时越有可能是Solutin What Happened When Gradient is Almost Zero即可视化梯度接近0时，Hessian矩阵中正值的比例与损失函数之间的关系，可以发现当征值比例很大时，loss都是很小的，不过我没找到怎么算Hessian，就没做这题。]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MLDS18(1) Why Deep Network?]]></title>
    <url>%2F2018%2F10%2F14%2FMLDS18-1-Why-Deep-Network%2F</url>
    <content type="text"><![CDATA[人道渺渺 仙道莽莽 鬼道乐兮 当人生门 仙道贵生 鬼道贵终 仙道常自吉 鬼道常自凶 高上清灵美 悲歌朗太空 唯愿仙道成 不欲人道穷&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-《度人经》 这是台大李宏毅程深度学习课程第一课的内容，这节课主要阐述了如何使用神经网络来拟合函数，即可以通过若干个线性整流函数(ReLU)组合形成分段函数，去逼近目标函数的各个部分。通过证明，可以得到浅层的神经网络可以拟合一切函数的结论。通过直观上比较和理论证明(没有看懂)，可以得到在相同参数（神经元）情况下，深层结构网络优于浅层网络结构的结论。 浅层神经网络是否可以拟合一切函数 给定一个L-Lipschitz function，即 ||f(x_1)-f(x_2)|| \le L||x_1-x_2|| 现在考虑在区间[0,1]上，使用一个只有一个隐藏层的网络，如果通过这个网络能找到一个拟合$f(x)$的函数$f^*(x)$，使 \sqrt{\int_0^1||f(x)-f^*(x)||^2dx}\le\epsilon $\epsilon$是一个很小的正数,那么拟合的目标就达到了。 如何才能做到呢？在几何直观上可以有以下关系 max_{0\le x\le 1}||f(x)-f^*(x)||\le \epsilon 是 \sqrt{\int_0^1||f(x)-f^*(x)||^2dx}\le\epsilon 的充分条件，所以只要做到第一个不等式，第二个不等式就满足了。设该函数被神经网络用$1/l$段分段函数来表示，根据第一条性质,那么有$l*L \le \epsilon$,所以$l \le \epsilon/L$,即需要$L/\epsilon$段分段函数才能满足要求。假设每一段函数需要用两个ReLU神经元来表示,那么就需要$2L/\epsilon$个神经元。 深度神经网络比浅层神经网络好多少 通过上述推导，可以发现浅层的神经网络的确可以拟合任何函数，但是使用Deep Structure会更有效率，输入相同参数的情况下，深层网络可以表示更多分段函数的片段。可以证明，如果使用取绝对值的神经元(线性整流函数)，每增加一个神经元，可以使原来的分段函数的段数翻倍。给一个宽度为K，深度为H的网络，可以表示至少$K^H$段分段函数。 $f(x)=x^2$考虑如上函数，如果在区间[0，1]要使用$2^m$个片段来拟合，可以算出浅层网络至少需要$O(1/\sqrt(\epsilon))$个神经元才能做到。如果是深度神经网络的话，可以发现使用m个取绝对值的神经元(两个ReLU)就可以表示，可以发现深度网络的优势很大。而有了这个表示之后，可以通过$x^2$表示$x_1,x_2$，即$\frac{1}{2}((x_1+x_2)^2-x_1^2-x_2^2)$,以此类推，可以表示$x^n$以及任意多项式函数。所以从这个角度来看，深层网络对于浅层网络具有很大优势。 TensorFlow学习使用tensorflow，参考了香港科技大学在Google Drive上分享的PPT，8012年了，我才初次接触深度学习框架，看着这么多花花绿绿的API还真是摸不到头脑。不过通过学习最简单的模型，我觉得tensorflow的使用流程大致可以分为以下几步： 设定数据源格式，一般使用tf.placeholder()方法定义，就是要把什么格式的数据倒进这堆矩阵进行训练。 设定训练过程中需要的参数，一般使用tf.Variable()方法定义 设定数据源如何与参数在矩阵中相互作用流动变化，最后得到我们对数据源的假设，比如预测数据源的标签 通过比较假设与真实数据的差别，设定损失函数 选定一种优化方法，设定学习速率去优化损失函数 通过以上定义，得到了一个计算图模型，现在初始化所有参数，开启tensorflow环境，通过run()方法传入数据源，并运行假设、损失函数、优化方法这些计算图，使参数得到更新，如果不出意外，通过多次迭代，当损失函数下降到一定程度后并经过交叉验证的考验后，我们需要的金丹(参数)就出炉了，然后就可以愉快的使用这些参数来对新数据进行预测了，不过如果丹炉的火焰久久不能化为天青色损失函数不能收敛，那么就要考虑是不是缺了什么玄学调参的步骤。 课堂作业课堂作业只做了一个。即使用具有相同参数的深层结构神经网络和浅层神经网络来拟合一个非线性函数，然后比较它们的收敛速度。代码放到了Github上。 下面贴一下结果(这两个网络好像都不是很好，运行了好多遍才得到勉强能看的结果),可以发现相同参数的情况下，浅层网络(1x9x1)刚开始很好，但是后劲不比深层的网络(1x4x3x1)：]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 主成分分析(Principal components analysis)]]></title>
    <url>%2F2018%2F09%2F29%2FCS229-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-Principal-components-analysis%2F</url>
    <content type="text"><![CDATA[今天是校庆，学校难得开灯，放一张图书馆的照片。 引入假如样本$x_i$代表一种类型的车，$x_i^{(1)}$，即第一个特征代表速度mile/h，$x_i^{(2)}$,第二个特征代表km/h，这两个特征是线性的，都只能表示速度这一信息，为了避免信息冗余，减少样本输入算法的计算量，我们可以在二者中去掉任意一个。但是在一些样本中，可能存在两个特征之间这样的相关性特别强，但是无法通过经验判断，此时就需要一个算法来自动对样本的维度进行缩减，同时尽可能保留有用的信息，这就是主成分分析法。 线性代数基础在推导算法之前，先复习一下需要用到的数学知识。 向量的投影与内积考虑有向量$\overrightarrow{A},\overrightarrow{B}$，其内积为 \overrightarrow{A}\overrightarrow{B}=|A|.|B|\cos(\overrightarrow{A},\overrightarrow{B})设$\overrightarrow{B}$的模为1,那么$\overrightarrow{A}$在$\overrightarrow{B}$上的投影为 |\overrightarrow{A}|\cos(\overrightarrow{A},\overrightarrow{B})基要描述一个向量，首先要确定一组基，然后给出向量在基所在方向上的投影值，如最普通的二维空间中向量(3,2)，它所在空间的基就是$ \begin{bmatrix} 1 \\ 0 \end{bmatrix} $与$ \begin{bmatrix} 0 \\ 1 \end{bmatrix} $,它们的方向分别是$x$轴和$y$轴,向量(3,2)在这两个方向上的投影分别是3和2。 矩阵乘法的意义(基变换)考虑普通二维空间中的向量(3,2)，使用一个矩阵$ \begin{bmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2}\\ -1/\sqrt{2}&amp; 1/\sqrt{2} \end{bmatrix} $去乘向量$\begin{bmatrix} 3 \\ 2 \end{bmatrix} $,我们会得到一个结果$\begin{bmatrix} 5/\sqrt{2} \\ -1/\sqrt{2} \end{bmatrix}$。现在通过下图来观察这个乘法到底做了什么。可以发现，结果就是向量$\begin{bmatrix} 3 \\ 2 \end{bmatrix}$在方向$\begin{bmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{bmatrix}$和方向$\begin{bmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \end{bmatrix}$上的投影，我们把这两个方向看作基的方向。所以，矩阵乘法的几何意义可以看作基变换，即改变基向量后，将原始空间中的向量转变为新空间中的向量。 主成分分析优化目标有一组$D$维向量，要将维度降为$d$，$d &lt; D$，降维操作，可以使用基变换来实现，即使用基向量矩阵$W$(d*D)来乘每一个样本$x_i$(D*1)，那么维度缩减后的新样本就是$y_i$(d*1)。但是基可以构造无数种，哪一种才是最好的呢？答案是寻求使新样本方差最大的基，要想说明这个问题十分复杂，直观的理解，方差越大，代表数据偏离中心的程度越大，由于维度减小，这些点已经越来越近了，这是没有办法避免的，如果维度变小后，再把方差变得更小，即把样本再变得更集中，这样只会丢失更多信息(极端的情况下可能会有更多的样本重叠)，所以为了在降维的同时尽量保留更多的信息，需要让样本更加偏离中心，即让方差最大化。 方差最大化设新空间的基向量矩阵为$W$，我们先考虑最简单的情况，即基向量矩阵$W$只有一个方向，那么他就只有D行1列,每个样本$x_i$的投影为$W^Tx_i$，样本的均值$\bar{x}=\frac{1}{N}\sum_{n=1}^NX_n$，那么投影后的新样本方差为 \frac{1}{N}\sum_{n=1}^N\{W^Tx_n-W^T\bar{x}\}^2=W^TSW$S$就是原样本的协方差矩阵 S=\frac{1}{N}\sum_{n=1}^N(x_n-\bar{x})(x_n-\bar{x})^T如果事先将新样本进行均值归一化，则均值为0(先减去均值再除以方差)，那么上面的公式将更好理解而且易用。 现在，我们要最大化$W^TSW$，同时有一个约束条件$W^TW=1$，此时运用拉格朗日乘子法既可以求解：构建拉格朗日函数： W^TSW+\lambda_1(1-W^TW)要最大化这个函数，首先对$W$求偏导置0最后发现SW=\lambda_1W出现了一个神奇的结果，这是一个特征向量和特征值的定义式，关于特征值和特征值的定义在线代中有详细描述，即对于矩阵$S$,$W$是一个特征向量，$\lambda_1$是对应的特征值，所以要使$W^TSW$最大，即让$\lambda_1$最大，所以在将原始样本投影到一个维度时，基向量的方向应该是协方差矩阵特征值最大的特征向量的方向，此时满足投影方差最大。那么如果基向量有两个，希望投影到两个方向上呢？假设有$W = \begin{bmatrix} W_1 &amp; W_2 \\ \end{bmatrix} $同理，使用拉格朗日乘子法,仍然有$SW=\lambda W$要让投影方差最大，有 W^TSW=\begin{bmatrix} W_1^T \\ W_2^T \end{bmatrix}S\begin{bmatrix} W_1 & W_2 \\ \end{bmatrix}=\begin{bmatrix} W_1^TSW_1 & W_2^TSW_1 \\ W_1^TSW_2 & W_2^TSW_2 \end{bmatrix}由于$SW=\lambda W$且S是实对称矩阵，此时因为有所有特征向量都是正交的结论，所以上面的协方差矩阵中，由于$W_i$是特征向量,互相正交，所以非对角位的值都是0，只剩下对角位上的方差，此时选取矩阵$S$前两个特征值最大的特征向量作为基向量，此时投影的方差是最大的。 如果要让方差最大，我出现了一个不算疑问的疑问，就是为什么不所有基向量都选特征值最大的特征向量呢？这是不可能的，因为基向量代表投影方向，都选取特征值最大的特征向量作为基向量，到最后还是只有一个方向。所以结合基向量是实对称矩阵的特征向量，这些基向量必定是正交的。 总结PCA是一种无参数技术，便于通用实现而无法个性化优化。至此，主成分分析的步骤是:有m条n维数据 将原始数据组成n行m列的矩阵X 将X的每一行进行0均值化 计算协方差矩阵$S = \frac{1}{m}XX^T$ 求出协方差矩阵的特征值和特征向量 如果要将矩阵规约为k维的矩阵，则选取前k个特征值最大的特征值对应的特征向量组成基向量矩阵P。 计算$Y=PX$即为降维到k维后的数据。 PCA的限制：无法应用于高阶相关性(非线性相关)、算法假设主成分位于正交方向，如果非正交方向存在方差较大的特征，PCA的效果大打折扣。 引用 主成分分析除开方差最大化这种理解，还有一种理解是基于最小重建误差来推导的，见《PRML》page 563。 知乎回答:如何理解主成分分析中的协方差矩阵的特征值的几何含义？描述了四层境界，我只能理解第一层。 PCA的数学原理:从几何层面详细解释了主成分分析的原理]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
        <tag>线性代数基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 因子分析(Factor Analysis)]]></title>
    <url>%2F2018%2F09%2F24%2FCS229-%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90-Factor-Analysis%2F</url>
    <content type="text"><![CDATA[CS229 因子分析(Factor Analysis)因子分析的本质是一种降维算法，即现有特征向量是由维度更小的因子经过映射得到的。当原有特征维度过高，甚至超过了样本数目，这时要运用高斯模型来建模是不可行的，因为特征的协方差矩阵是奇异的，所以可以利用因子分析对数据进行降维，即将隐变量(因子)和观测到的样本的联合分布进行建模，得到样本的边缘分布，由于直接求解边缘分布的最大似然估计没有封闭解(参数是因子到样本的映射中涉及到的系数)，所以运用EM算法得到一个关于因子到样本映射的参数的解，得到映射的参数后，就可以将所有样本还原为维度更小的因子了。 引入给定一个数据集$x^{(i)}\in{R^n}$，假设它们是来自几个高斯分布的组合。那么就可以使用上一篇文章的EM算法来对数据建模。假如$n &gt;&gt; m$，应用到一个高斯模型可能比较困难，更不用说应用到高斯分模型，对于一个高斯分模型,如果使用最大似然估计。(这里开一个坑，就是这个最大似然估计好像涉及到关于矩阵求导，我想试一下，最后还是放弃了，不过第一个结果没有涉及到矩阵，很容易就出来了) \mu = \frac{1}{m}\sum_{i=1}^mx^{(i)}\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T现在观察这个结果，可以发现当$m &lt;&lt; n$时，协方差矩阵$\Sigma$是奇异的，奇异矩阵是不可逆的，为什么不可逆呢？因为它的行列式为0，而行列式为0的在几何中的直观意义是什么呢？考虑有如下矩阵： \begin{bmatrix} a & b \\ c & d \\ \end{bmatrix}现在要求他们的逆，设逆矩阵为: \begin{bmatrix} A & B \\ C & D \\ \end{bmatrix}所以有 \begin{align} aA + bC = 1\\ aB + bD = 0 \\ cA + dC = 0 \\ cB + dD = 1 \end{align}将(1)(3),(2)(4)分别联立就可以进行求解。而如果原矩阵行列式为0，则有$ad = bc$，即$\frac{a}{c}=\frac{b}{d}$，那么上面联立的方程所表示的线在几何空间中肯定是平行的，所以这个方程肯定无解或者有无数解，所以行列式为0的矩阵肯定不可逆。(不过我在这里只列举了2*2矩阵的情况，泛化还需要进一步证明)。另外，为什么$m &lt;&lt; n$时，行列式为0呢？这一点我也想了很久，但是google也google不到详细的证明过程，网上的博客都很自然引出了这个结论，最后我还是只有举例验证了一下，最后发现真的是这样(菜到安详.jpg)。 好的，绕了这么大一个圈子，终于理清了为什么$m &lt;&lt; n$的情况下矩阵的奇异的。现在回过头来看高斯模型中，参数$\Sigma$参与了什么，观察公式可以发现正是要用到$\Sigma^{-1}$以及$\frac{1}{|\Sigma|^{1/2}}$，如果$\Sigma$奇异，那么这两个都没有办法得到。所以，在$m &lt;&lt; n$这种情况下，就可以使用因子分析对数据进行降维打击处理，然后就可以继续快乐地进行建模了。在这之前，还可以对$\Sigma$进行限制来建模，即将其限制为对角矩阵，甚至还把对角元素的值都设为一样的，有了这样的限制，因为协方差矩阵反映的就是各个维度之间的相关性，非对角元素为0，特征的各个维度之间在高斯模型中已经是互相独立的了，所以可以看到高斯分布的对称轴和坐标轴平行，如果对角元素还是一致的，高斯分布的轮廓就是一个圆形(二维中)了，通常情况下这种方式都是有效的，但是如果不使用这个方法呢，就要用因子分析模型了。 高斯模型的边缘分布和条件分布这是推导算法所需要的数学工具考虑有一个随机向量 \begin{bmatrix} x_1 \\ x_2 \\ \end{bmatrix}$x_1 \in R^r,x_2 \in R^s,x \in R^{r+s}$假设x \sim \mathcal{N}(\mu,\Sigma) \mu = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \end{bmatrix}, \Sigma = \begin{bmatrix} \Sigma_{11} &\Sigma_{12}\\ \Sigma_{21} & \Sigma_{22} \\ \end{bmatrix}有$\mu_1 \in R^r,\mu_2 \in R^s,\Sigma_{11} \in R^{r*r}，\Sigma_{12} \in R^{r*s}$，以此类推。另外，协方差矩阵是对称的，$\Sigma_{12} = \Sigma_{21}^T$对于$x_1,x_2$这个联合多元高斯分布的的边缘分布和条件分布，姑且作为结论性的东西直接用吧，贴图片啦： 其边缘分布为：$x_1 \sim \mathcal{N}(\mu_1,\Sigma_{11})$ 条件分布为:$x_1|x_2 \sim \mathcal{N}(\mu_{1|2},\Sigma_{1|2})$ 因子分析模型假设一个关于$(x,z)$的联合分布，$z\in R^k$是一个随机隐变量： z \sim \mathcal{N}(0,I)\\ x|z \sim \mathcal{N}(\mu+\Lambda z，\Psi)$\mu \in R^n,\Lambda \in R^{n*k}$,对角矩阵$\Psi \in R^{n*n}$,$k$通常比$n$更小。现在，可以想象一下每个数据点是通过什么方式生成的：在一个k维多元高斯分布生成一个样本$z^{(i)}$，然后把它映射到一个n维空间通过$\mu+\Lambda z^{(i0}$，最后加上协方差$\Psi$的噪声。同样的，因子分析模型可以被定义为： z \sim \mathcal{N}(0,I) \\ \epsilon \sim \mathcal{N}(0,\Psi) \\ x = \mu + \Lambda z + \epsilon其中$\epsilon$和$z$是独立的。变量$z$和$x$服从联合高斯分布 \begin{bmatrix} z \\ x \\ \end{bmatrix} \sim \mathcal{N}(\mu_{zx},\Sigma)现在，要求解$u_{zx}$和$\Sigma$已知$E[z]=0$，所以 \begin{align} E[x] &= E[\mu+\Lambda z+ \epsilon]\\ &=\mu + \Lambda E[z] + E[\epsilon] \\ &= \mu \end{align}所以有\mu_{zx} = \begin{bmatrix} \overrightarrow{0} \\ \mu \\ \end{bmatrix}接下来要计算$\Sigma$，需要计算 \Sigma_{zz} = E[(z - E[z])(z-E[z]^T)] \\ \Sigma_{zx} = E[(z-E[z])（x-E[x]^T)] \\ \sigma_{xx} = E[(x-E[x])(x-E[x])^T]因为$z \sim \mathcal{N}(0,I)$，可以得到$E_{zz}=Cov(z)=I$ \begin{align} E[(z-E[z])(x-E[x])^T] &= E[z(\mu + \Lambda z+\epsilon-\mu)^T] \\ &= E[zz^T] \Lambda^T + E[z\epsilon^T] \\ & = \Lambda^T \end{align}在最后一步，因为$z$和$\epsilon$是独立的，$E[z\epsilon^T] = E[z]E[\epsilon^T]=0$ \begin{align} E[(x-E[x])(x-E[x])^T] &= E[(\mu+\Lambda z+\epsilon -\mu)(\mu + \Lambda z+\epsilon-\mu)^T] \\ &= E[\Lambda zz^T\Lambda^T + \epsilon z^T\Lambda^T + \Lambda z\epsilon^T + \epsilon\epsilon^T] \\ & = \Lambda E[zz^T]\Lambda^T + E[\epsilon\epsilon^T] \\ & = \Lambda\Lambda^T + \Psi \end{align}所以 \begin{bmatrix} z \\ x \\ \end{bmatrix} \sim \mathcal{N}(\begin{bmatrix} \overrightarrow{0} \\ \mu \\ \end{bmatrix}),\begin{bmatrix} I&\Lambda^T \\ \Lambda & \Lambda\Lambda^T + \Psi\\ \end{bmatrix}$x$的边缘分布为：$x\sim \mathcal{N}(\mu,\Lambda\Lambda^T+\Psi)$关于参数的最大似然估计为： \ell(\mu,\Lambda,\Psi)=log\prod_{i=1}^m\frac{1}{(2\pi)^{\frac{n}{2}}}exp(-\frac{1}{2}(x^{(i)}-\mu)^T(\Lambda\Lambda^T+\Psi)^{-1}(x^{(i)}-\mu))这个最大似然估计没有算法可以求出封闭解，所以使用EM算法来解决。 将EM算法运用到因子分析将EM算法运用到因子分析中,$z$是隐变量 E步Q_i(z^{(i)})=p(z^{(i)}|x^{(i)};\mu,\Lambda,\Psi),运用之前多元高斯模型条件分布的公式： 所以： M步 可以被简化为： 后续最大化过程再看讲义吧，推不下去了ORZ。]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法]]></title>
    <url>%2F2018%2F09%2F15%2FEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Jensen’s inequality这一部分引入了Jensen不等式来导出EM算法，利用jensen不等式取等号的条件构造似然函数的下界，并且可以证明EM算法是可以收敛的。最后一部分将算法代入高斯混合模型求解。 convex function(凸函数)设$f$是一个值域为实数的的函数，如果对于$x\in R$有$f’’(x)\ge0$，那么$f$是一个凸函数。在$f$的输入是向量化的值的情况下，那么该函数的Hessian矩阵是半正定的($H\ge0$)，如果二阶导是大于0的，该函数就是“strictly convex”的，输入是向量化的情况下，Hessian矩阵就是严格半正定的。 公式如果$f$是一个凸函数，设$X$为随机变量，则有 E[f(x)]\ge f(E(X))另外，如果$f$是”strictly convex”的，那么当且仅当$X=E[X]$的概率为1时，等式$E[f(x)]=f[E(X) ]$成立，由于表示期望可以删除括号的约定，$f(EX)=f(E[X])$ 为了对该不等式有一个直观的理解，可以用下图来说明： Remark：如果函数是一个凹函数，那么不等式为$E[f(x)]\le f[E(X)]$ EM算法建模问题假如有由m个独立样本组成的训练集$\{x^{(1)},…,x^{(m)}\}$，现在希望通过模型$p(x,z)$对数据进行建模，通过最大似然估计拟合参数： \ell(\theta)=\sum_{i=1}^mlogp(x;\theta)=\sum_{i=1}^{m}log\sum_zp(x,z;\theta)但是，直接求解最大似然估计可能很困难，因为$z^{(i)}$是一个随机隐变量，如果能够观测到$z^{(i)}$的话，求解将会变得容易。 求解思路E步：重复构造一个似然函数的下界M步：进一步优化这个下界的取值 利用jensen不等式： \begin{align} \sum_i logp(x^{(i)};\theta) &= \sum_{i}log_{z^{(i)}}p(x^{(i)},z^{(i)};\theta) \\ &=\sum_ilog\sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\\ &\ge \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \end{align}证明过程：有$X_i=\frac{p(x_{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \in X,i=1,2,…,m$,在这个式子中，$X_i$的取值受未知的$z^{(i)}$和$Q(Z^{(i)})$所影响，现在设$z^{(i)}$的分布函数就是$Q(z^{(i)})$，所以整个式子的值只受$z^{(i)}$所影响。设$f(X)=logX$那么(第二步)，$X_i$的期望$E(X_i)=\sum_{z^{(i)}}Q_i(z^{(i)})X_i=\sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x_{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$所以，$f(E(X_i))=log\sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x_{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$另外(第三步)，$f(X_i)$的期望$E(f(X_i))=\sum_{(z^{(i)})}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$由于$f(X)=logX$的二阶导小于0，所以该函数是一个凹函数，所以有$f(E(X_i))\ge E(f(X_i))$，综上，每一个$X_i$的期望对应的函数值都大于$f(X_i)$的期望，上式得证。 $Q_i(z^{(i)})$的选择现在，对于任何分布函数$Q_i$，公式3都对最大似然估计$\ell(\theta)$限定了一个下界。为了让估计更准确，要尽量让似然函数与下界更接近。根据jensen不等式的性质，当且仅当$f(X)$的输入都是一个常量时，等号才成立，当等号成立时，此时的下界是最接近的似然函数的，所以，要求不等式的输入为一个常量，使等式成立。 \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=c要使上式成立，易得: Q_i(z^{(i)})\propto p(x^{(i)},z^{(i)};\theta)由于$\sum_zQ_i(Z^{(i)})=1$，可得: \begin{align} Q_i(z^{(i)}) &= \frac{p(x^{(i)},z^{(i)};\theta)}{\sum_zp(x^{(i)}),z;\theta)} \\ &=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)});\theta} \\ &=p(z^{(i)}|x^{(i)};\theta) \end{align}因此，将$Q_i$设为给定$x^{(i)}$后$z^{(i)}$的后验分布。 算法导出E步,对于每一个i: Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta)M步: \theta:=arg\quad max_\theta\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}算法收敛的证明不断重复以上两步，直到算法收敛，如何保证算法是收敛的呢？假设$\theta^{(t)}$和$\theta^{(t+1)}$是两次连续迭代的参数，由于$Q_{i}$的选择使式(3)的等号成立，所以有: \ell(\theta^{(t)})=\sum_i\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^t)}{Q_i^{(t)}(z^{(i)})}由于$\theta^{(t+1)}$是用来最大化上式的，因此有: \begin{align} \ell(\theta^{t+1}) &\ge \sum_i\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{t+1})}{Q_i^{(t)}(z^{(i)})} \\ &\ge \sum_i\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^t)}{Q_i^{(t)}(z^{(i)})} \\ &=\ell(\theta^{(t)}) \end{align}第一个不等式来自于公式(3)，为什么不是等号呢，因为$Q_i$的参数是$\theta^t$,而第二个不等式成立的原因是因为在M步中，$\theta^{(t+1)}$正是通过最大化 \sum_i\sum_{z^{(i)}}Q_i^t(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i^t{z^{(i)}}}得到的，所以第一个式子的值必定大于等于第二个式子的值，因此该算法必定会收敛。 Remarks如果定义 J(Q,\theta)=\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}此时EM算法可以看作一个坐标上升算法。 高斯混合模型E步：计算 w_j^{(i)}=Q_i(z^{(i)=j})=P(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)M步:我要贴图片啦，反正就是把$Q_i(z^{(i)})$和高斯分模型表示出来代入原始模型，如下： 现在要更新$\mu_l$，对上式求偏导置为0，解得,依旧是贴图片:&lt;/div&gt;&lt;/div&gt;要更新$\phi_j$，把由$\phi_j$决定的项加起来，发现我们需要最大化：&lt;/div&gt;&lt;/div&gt;由于有一个所有$\phi_j$的和为1的限制，所以可以构造拉格朗日函数：&lt;/div&gt;&lt;/div&gt;$\beta$是拉格朗日乘子，求偏导置0，解得&lt;/div&gt;&lt;/div&gt; \phi_j=\frac{\sum_{i=1}^mw_j^{(i)}}{-\beta}所以\sum_j\phi_j=1=\frac{\sum_{i=1}^m\sum_{j=1}^kw_j^{(i)}}{-\beta}又因为 \sum_jw_j^{(i)}=1所以\sum_{i=1}^m1=-\beta所以\phi_j:=\frac{1}{m}\sum_{i=1}^mw_j^{(i)}]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[混合高斯模型与EM算法]]></title>
    <url>%2F2018%2F09%2F13%2F%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E4%B8%8EEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Mixtures of Gaussians这一部分引入了高斯混合模型和EM算法的定义，但是没有进一步证明EM算法的导出。 引入给定训练集$\{x^{(1)},…,x^{m}\}$，根据无监督学习的定义，这些数据不带有任何标签。现在通过一个联合分布对数据进行建模: p(x^{(i)},z^{(i)})=p(x^{(i)}|z^{(i)})p(z^{(i)})其中 $z^{(i)}\sim Multinomial(\phi),p(z^{(i)}=j)=\phi_j \ge 0,\sum_{j=1}^{k}\phi_j=1$ $x^{(i)}|z^{(i)} = j \sim \mathcal{N}(u_j,\Sigma_j)$ k表示$z^{(i)}$取值的个数，所以模型假设每个$x^{(i)}$是由随机选择(多项分布)的$z^{(i)}$生成的，且$x^{(i)}$来自由$z^{(i)}$所对应的高斯分布。这就叫做高斯混合模型。$z^{(i)}$被称作隐变量，这正是让根据观测数据预测模型参数$(\phi,\mu,\Sigma)$的困难所在。 最大似然估计为了估计高斯混合模型的参数，其最大似然估计可以被表示为 \begin{align} \ell(\phi,\mu,\Sigma) &= \sum_{i=1}^{m}logp(x^{(i)};\phi;\Sigma) \\ & = \sum_{i=1}^m log \sum_{z^{(i)}=1}^kp(x^{(i)}|z^{(i)};\mu;\Sigma)p(z^{(i)};\phi) \end{align}第一行到第二行的转变表示每个观测数据$x^{(i)}$被模型生成的概率是每个高斯分模型生成$x^{(i)}$的概率之和。但是，将偏导置为0后会发现，最大似然估计的参数没有封闭解。那么，如果已知$z^{(i)}$的值，即已经确定$x^{(i)}$来自于哪一个高斯分模型，则最大似然估计为 \ell(\phi,\mu,\Sigma)=\sum_{i=1}^{m}logp(x^{(i)}|z^{(i)};\mu,\Sigma)+logp(z^{(i)};\phi)最大化这个似然函数： \phi_j = \frac{1}{m}\sum_{i=1}^m1\{z^{(i)}=j\}\mu_j = \frac{\sum_{i=1}^m1\{z^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m1\{z^{(i)}=j\}}\Sigma_j=\frac{\Sigma_{i=1}^m1\{z^{(i)}=j\}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m1\{z^{(i)}=j\}}如果$z^{(i)}$是已知的，最大似然估计的结果和高斯判别分析非常相似。如果是未知的，就需要使用EM(Expectation Maximization)算法来解决该问题。 EM算法EM算法分为两个主要步骤，在上述问题中，在E步中，算法将“猜测”$z^{(i)}$的取值，在M步中，算法将根据猜测的$z^{(i)}$更新高斯混合模型的参数。E步: w_j^{(i)}:=p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)M步： \phi_j = \frac{1}{m}\sum_{i=1}^mw_j^{(i)}\mu_j = \frac{\sum_{i=1}^mw_j^{(i)}x^{(i)}}{\sum_{i=1}^mw_j^{(i)}}\Sigma_j=\frac{\Sigma_{i=1}^mw_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m1\{z^{(i)}=j\}}在E步中，在给定$x^{(i)}$和模型参数下，计算$z^{(i)}$的后验概率。例如，使用贝叶斯公式： p(z^{(i)}=j|x^{(i)};\phi,\mu,\Sigma)=\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}其中$p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)$来自给定$u_j,\Sigma_j和x^{(i)}$之后的高斯分布，$p(z^{(i)}=j;\phi)$来自$\phi_j$。 现在比较已知$z^{(i)}$和未知的情况，如果已知，那么只有一个高斯模型，每个数据点都来自该模型，所以可以求解，如果未知，则不知道每个数据点到底来自那个分模型，所以只有使用EM算法(待证明) 和K-means法类似，EM算法有出现局部最优的可能，所有多次初始化参数会有更好的表现。现在对于该算法还有一些问题，即算法是如何推导的？如何保证该算法会收敛？,需要在接下来证明。]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Note5 Learning Theory 2]]></title>
    <url>%2F2018%2F08%2F19%2FCS229-Note5-Learning-Theory-2%2F</url>
    <content type="text"><![CDATA[规则化与模型选择这部分的内容主要是模型选择、特征选择以及使用参数的最大后验概率来规避最大似然估计可能出现的过拟合问题。 模型选择的主要方法有保留交叉验证、k重交叉验证、留1交叉验证。 特征选择的主要方法有封装特征选择（前向搜索与后向搜索的启发式算法）、过滤特征选择（计算特征与标签之间的互信息进行排名的启发式算法） 最大后验概率是最大似然估计与贝叶斯估计融合的产物，最大似然估计与贝叶斯估计分别是频率学派与贝叶斯学派的代表，目前内容所涉及到的区别主要是频率学派认为模型的参数是一个固定不变的未知值，而贝叶斯学派认为参数是一个具有先验分布的未知值，在知乎上看到一个回答很有趣，其大意为你打麻将的时候，根据麻将规则，某人P有一张牌A，如果他出了胜算更大，但是根据观察他没有出这张牌，如果你是是频率学派，那么你会单纯的根据你所已经观察到的牌去推断你摸到牌A的概率，即P是没有这张牌的，没有出现的牌一定还在剩余的牌里，如果你是贝叶斯学派，除了观察已经出现的牌，你还会根据别人所打出的牌（对方是否有什么套路，有牌但是还没打出），打牌人之间的关系(是否有喂牌情况，故意让别人所以不出)去得到摸出牌A的先验概率，不断修正你摸到牌A的概率。当然，这些估计方法与学派之间的区别不仅是这样，有必要专门学习一下。 交叉验证如果给定训练集$S$,模型集合$M$中有着不同的假设，选取经验风险最小化的假设不是合理的解决办法，因为存在过拟合的情况。为了避免出现这种情况，有以下几种模型选择方法。 保留交叉验证保留交叉验证(hold-out cross validation)又叫做简单交叉验证，算法步骤如下： 将训练集随机划分，70%z作为$S_{train}$训练集，30%作为$S_{CV}$交叉验证集。 使用$S_{train}$训练模型，获得对应的假设$h_i$ 选择并输出在交叉验证机上既有最小误差的假设$h_i$ （可选步骤）重新使用整个训练集对模型$M_i$训练假设$h_i$ 缺点：如果数据本来就比较少，将30%数据作为交叉验证集，会对训练效果造成较大影响 K重交叉验证K重交叉验证(k-fold cross validation)的目的就是为了解决保留交叉验证在数据比较少时效果不好的缺点。 随机将数据划分为k份，记作$S_1,…,S_k$ 对于每个模型$M$，其评估方法如下：每个模型都训练k次，从i=1循环至i=k，每次使用除开$S_i$的数据作为训练集，使用$S_i$作为测试集合，其误差记作$\hat{\mathcal{E}}(h_i)$。经过k次循环后，记录这个模型的平均训练误差。 选择具有最小平均训练误差的模型$M$,使用整个训练集进行训练，最后得到的假设就是最终结果。 留1交叉验证当数据实在十分稀少的时候，在k重交叉的基础上，令$k=m$。 特征选择对模型给定n个特征，有$2^n$中可能的特征子集，当$n$非常大的时候，对每一种特征子集都训练一遍的代价是非常大的，因此使用一种启发式的方式去寻找合适的特征子集。 foward search算法 初始化$\mathcal{F}=\phi$ Repeat$\{ \\ (a)for \quad i=1,…,n,如果i \notin \mathcal{F}，令\mathcal{F_i}=\mathcal{F}\cup\{i\}使用交叉验证去评估特征\mathcal{F_i}\\ (b)将上一步得到的最好的特征子集赋值给\mathcal{F}\\ \}$ 选择并输出整个过程中最好的特征子集 在该算法中，外部循环停止的条件可以是当$\mathcal{|F|}=n$时，也可以是自己设定的一个界限。该算法的实例化被称作封装特征选择(wrapper model feature selection),除了foward search，还有一种被称作backward search的算法，其将$\mathcal{F}$初始化为包含全部特征的全集，每次循环从中去除一个特征，直到$\mathcal{F}=\phi$。 这个算法不是实时更新最佳特征集合的，每次循环的最佳子集都会参与最后的特征选择。算法的缺点：计算复杂度较大，一个完整的foward search算法会调用$O(n^2)$次学习算法。 过滤特征选择(Filter feature selection)过滤特征选择的目的是为了在启发式的基础上让计算代价降低。其基本思想是计算每个特征$x_i$对标签$y$的影响程度，记作$S(i)$，然后根据其排名来决定优先选择哪些特征。具体的，使用MI(mutual infomation)互信息作为$S(i)$,其计算公式为： MI(x_i,y)=\sum_{x_i\in\{0,1\}}\sum_{y\in\{0,1\}}p(x_i,y)log\frac{p(x_i,y)}{p(x_i)p(y)}上式中所需的概率都可以从训练集中的经验分布得到。互信息还可以用相对熵(KL散度)来表示。 贝叶斯统计和正则化贝叶斯正则化是用来防止过拟合的工具。 最大似然估计（Maximum likelihood）在此前推导线性回归和逻辑回归的代价函数时，都是通过最大似然估计ML得到的优化函数。 \theta_{ML}=arg\quad max_\theta \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)关于最大似然估计，这是之前的记录：似然函数，最大似然估计来源于概率中的频率学派(frequentist),其观点是对于一组数据，基于一个模型去拟合参数时，参数$\theta$本身没有概率分布，它是一个未知的常量，通过对观测到的事件进行最大似然估计得到的$\theta$就是最符合已知训练数据的参数。例子：抛一枚硬币10次，观测到9正1反，设正面向上的概率为p，那么使用最大似然估计，9正1反的概率为$p^9(1-p)$，对此概率最大化，此时对应的p就是通过最大似然估计得到的参数。 贝叶斯估计与最大后验概率（Maximum A Posteriori）贝叶斯估计是属于贝叶斯学派拟合参数的方法，与最大似然估计相反，在拟合参数时，它认为$\theta$是本身有着先验概率分布的未知值，要做的工作就是在知道观测结果$S$时，哪一个$\theta$的概率最大，这个概率可以通过贝叶斯统计得到。 \begin{align} p(\theta|S)&=\frac{p(S|\theta)p(\theta)}{p(S)} \\ &=\frac{(\prod_{i=1}^mp(y^{(i)}|x^{(i)},\theta))p(\theta)}{\int_\theta(\prod_{i=1}^mp(y^{(i)}|x^{(i)},\theta)p(\theta))d\theta} \end{align}例子：抛一枚硬币10次，观测到9正1反，设正面向上的概率为p，求在事件S为9正1反的条件下，使概率$p(p|S)$取最大值的p。 如何使用贝叶斯估计的后验概率：已知数据$S$,要预测一个新的样本$x$的标签$y$，即在已知数据$S$的情况下计算新样本标签$y$的数学期望。 \begin{align} E[y|x,S]=\int_yyp(y|x,S)dy \end{align}此时需要求$p(y|x,S)$，这个式子隐藏了一个含义，y的分布与$\theta$有关，而$\theta$服从于某种分布，因此考虑所有$\theta$可能的情况，使用全概率公式： \begin{align} p(y|x,S)=\int_\theta p(\theta,y|x,S)d\theta \end{align}运用条件概率： \begin{align} p(\theta,y|x,S)=p(y|x,\theta,S)p(\theta|S) \end{align}其含义为：已知数据为S的条件下，参数为$\theta$且$x$的标签为$y$的概率=已知数据S和参数为$\theta$的条件下$x$的标签为$y$的概率乘以数据为S的条件下参数为$\theta$的概率同时，有化简p(y|x,\theta,S)=p(y|x,\theta)这是因为对于预测$x$,当参数$\theta$确定后，数据S如何分布与x的标签是什么已经没有关系了，x的标签的取值概率完全由参数和模型决定。将（4）代入（3）得到： \begin{align} p(y|x,S)=\int_\theta p(y|x,\theta)p(\theta|S)d\theta \end{align}现在，如果$\theta$确定，则$p(\theta|S)$确定（使用公式2），$p(y|x,\theta)$也确定，计算出$p(y)$，代入公式3，那么标签y的数学期望也就求出，完成预测。 贝叶斯估计的缺点由于参数是随机分布的，在公式2中进行积分是非常困难的。通过观察公式2，可以发现分母是一个归一化因子，因此可以不必计算。于是使用 \theta_{MAP}=arg\quad max_{\theta}\prod_{i=1}^{i=m}p(y^{(i)}|x^{(i)},\theta)p(\theta)上式过程即为最大后验概率，它属于最大似然估计与贝叶斯估计互相融合的方法，一方面具有点估计的特征，另一方面考虑了参数的先验概率$p(\theta)$，通常$p(\theta)$这个先验概率被定义为高斯分布$\theta~\mathcal{N}(0,\tau^2I)$，通过贝叶斯估计的最大后验概率选择的参数与最大似然估计选择的参数相比，过拟合的程度会更小。]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>Learning Theory</tag>
        <tag>概率论基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Note4 Learning Theory]]></title>
    <url>%2F2018%2F07%2F27%2FCS229-Note4-Learning-Theory%2F</url>
    <content type="text"><![CDATA[CS229 Note4 Learning Theory这一部分的内容： 引入偏差和方差(泛化误差) 引入训练误差、经验风险最小化(ERM) 在有限假设类以及无限假设类中进行ERM时，需要考虑的问题(样本复杂度以及假设类大小对总体训练误差的影响) 偏差/方差 平衡(bias/variance)泛化误差模型假设对于不一定在训练集内的样本进行预测的误差 偏差(bias)即使使用大量数据进行训练，模型也不能很好的捕捉到数据的规律，存在很大的泛化误差。(对训练集中的样本和非训练集样本都不能很好预测，欠拟合) 方差(variance)方差是泛化误差的另一种表现形式，即模型可能只是碰巧与当前有限的训练集的规律相符合，但是对于更大的训练集中的数据，可能存在相应的预测误差。(对训练集中的样本能够很好预测，对于非训练集样本不能很好预测，过拟合) 一个好的模型，需要在以上两种泛化误差之间寻求一个平衡 引理union bound设$A_1$,$A_2$,$A_3$,$…$,$A_k$是K个不相同事件(不一定互相独立)，那么 P(A_1\cup...\cup A_k)\leq P(A_1)+...+P(A_k)Hoeffding inequality设$Z_1$,$…$,$Z_m$是从伯努利分布$\phi$中抽取的m个独立同分布的随机变量，即$P(Z_i=1)=\phi$,$p(Z_i=0)=1-\phi$。令$\hat\phi=(1/m)\sum_{i=1}^mZ_i$,即随机变量的平均值，当m很大时，$\hat\phi$可以很好的估计真实的$\phi$值，令$\gamma&gt;0$，那么有 P(|\phi-\hat\phi|>\gamma)\leq2exp(-2\gamma^2m)该引理也被称作Chernoff bound。 training error(empirical error)给定训练集$S$，大小为m,训练样本$(x^{(i)},y^{(i)})$遵从某种独立同分布的概率分布$\mathcal{D}$，对于一个假设$h$,定义训练误差或者经验风险(empirical risk)为： \hat{\mathcal{E}}(h)=\frac{1}{m}\sum_{i=1}^m1\{h{x^{(i)}}\neq y^{(i)}\}上式表示：如果从分布$\mathcal{D}$中取出一个新样本$(x,y)$，$h$预测失误的概率。 如果有一个线性分类器$h_\theta(x)=1\{\theta^Tx\geq0\}$，如何去拟合参数$\theta$？一种方式就是最小化训练误差，选择使训练误差最小那个参数$\hat\theta$ \hat\theta = arg \quad \min_\theta \hat{\mathcal{E}}(h_\theta)这个过程就是经验风险最小化，empirical risk minmization(ERM)，学习算法输出的假设是$\hat{h}=h_{\hat{\theta}}$,ERM算法是最“基础”的算法，逻辑回归算法也可以看作是ERM算法的近似。 我们把学习算法的所有假设的集合记作假设类$\mathcal{H}$,$\mathcal{X}$是所有输入特征的取值域，ERM现在是在假设类$\mathcal{H}$上进行最小化，学习算法将会选取风险最小那个假设$\hat{h}$： \hat{h}=arg\quad min_{h \in \mathcal{H}}\hat{\mathcal{E}}(h)有限假设类$\mathcal{H}$设假设类$\mathcal{H}=\{h_1,…,h_k\}$包含$k$个假设。假设函数$h$是将输入$x$到${0,1}$的映射。EMR将要选取具有最小风险的$\hat{h}$ 设伯努利所及变量$Z$的分布如下，样本点$(x,y)$服从分布$\mathcal{D}$，设$Z=1\{h_i(x)\neq y\}$，$Z_j = 1\{h_i(x^{(j)})\neq y^{(j)}\}$,因为训练集是独立同分布的，因此$Z$和$Z_j$也是一样的分布。 误分类概率$\mathcal{E}(h)$恰好是$Z$(和$Z_j$)的期望，训练误差写作如下： \hat{\mathcal{E}}(h_i)=\frac{1}{m}\sum_{i=1}^mZ_j现在对这个概率应用Hoeffding inequality P(|\mathcal{E}(h_i)-\hat{\mathcal{E}}(h_i)|>\gamma)\leq 2exp(-2\gamma^2m)从上式可以可以得到，假如m足够大，那么对于特定的$h_i$，训练误差与泛化误差有很大概率接近。要证明其对于所有$h$的一般性，令$A_i$代表事件$|\mathcal{E}(h_i)-\hat{\mathcal{E}}(h_i)|&gt;\gamma$，由于已经有了不等式$P(A_i)\leq2exp(-2\gamma^2m)$，那么使用union bound。 \begin{align} P(\exists h\in \mathcal{H}.|\mathcal{E}(h_i)-\hat{\mathcal{E}(h_i)}|>\gamma) &= P(A_1 \cup ... \cup A_k)\\ &\leq \sum_{i=1}^{k}P(A_i) \\ &\leq\sum_{i=1}^k2exp(-2\gamma^2m) \\ &= 2kexp(-2\gamma^2m) \end{align}如果上式两边同时被1所减 \begin{align} P(\neg h \in \mathcal{H}.|\mathcal{E}(h_i)-\hat{\mathcal{E}}(h_i)|>\gamma) &= P(\forall h \in \mathcal{H}.|\mathcal{E}(h_i)-\hat{\mathcal{E}}(h_i)|\leq\gamma|) \\ \geq 1-2kexp(-2\gamma^2m) \end{align}上式表示，对于所有假设$h$,$\mathcal{E}(h)$比$\hat{\mathcal{E}}(h)$小$\gamma$的概率至少是$1-2kexp(-2\gamma^2m)$,这叫做一致收敛(union convergence) 在上式中，给定$m$,$\gamma$和误差概率(即所有泛化误差-训练误差小于等于$\gamma$的概率)，可以对剩余的一个变量进行边界限定。 令$\delta = 2kexp(-2\gamma^2m)$，解以上不等式可以得到： m \geq \frac{1}{2\gamma^2}log\frac{2k}{\delta}$m$被称为样本复杂度，这个边界表示已知误差概率和$\gamma$时，至少需要多少个训练样本。 当$m$和误差概率固定时，可以解得 |\hat{\mathcal{E}}(h)-\mathcal{E}(h)| \leq \sqrt{\frac{1}{2m}log\frac{2k}{\delta}}那么，如何最去最小化训练误差并选取合适的$\hat{h}$呢？设$h^{\ast}$是使泛化误差最小的假设，$\hat{h}$是使训练误差最小的$h$，有： \begin{align} \mathcal{E}(\hat{h})&\leq\hat{\mathcal{E}}(\hat{h}) + \gamma \\ &\leq \hat{\mathcal{E}}(h^{\ast}) + \gamma \\ &\leq {\mathcal{E}}(h^{\ast}) + 2\gamma \end{align}第一行使用了一致收敛假设$|\mathcal{E}(\hat{h})-\hat{\mathcal{E}}(\hat{h})| \leq \gamma$ 对于第二行，因为对于所有$h$，有$\hat{\mathcal{E}}(\hat{h}) \leq \hat{\mathcal{E}}(h)$。 对于第三行，再一次使用了一致收敛假设。 现在，有以下定理：给定$|\mathcal{H}|=k$,以及$m$,$\delta$，误差概率至少是$1-\delta$,于是有： \mathcal{E}(\hat{h})\leq (min_{h\in \mathcal{H}}\mathcal{E}(h))+2\sqrt{\frac{1}{2m}log\frac{2k}{\delta}}当假设类的假设变多，上式的第一项代表偏差只会变小，但是第二项代表方差，k会变大,整体也会变大，因此在需要在这两者之间寻求平衡。 无穷假设类$\mathcal{H}$在无穷假设类中，我们能够得到类似于有限假设类中的结论吗？首先来看一个看似不那么“正确”的论证。假如假设有d个参数，在电脑中，每个参数采用双精度浮点数存储，每个双精度浮点数的大小为64bit，那么参数的总共大小为64dbit,每个bit可以取值0或者1，那么总共有$k=2^{64d}$种假设。根据上一节的推论有 m\geq O(\frac{1}{\gamma^2}log\frac{2^{64d}}{\delta})=O(\frac{d}{\gamma^2}log\frac{1}{\delta})=O_{\gamma,\delta}(d)所以，m至少与参数个数是线性关系。 上述论点存在缺陷，为了得到更令人满意的论点，引入以下概念 VC维给定一个集合$S=\{x^{(1)},…,x^{(i)}\}$,$x^{(i)} \in \mathcal{X}$,如果在假设类中存在假设$h$，使S中的点具有的标签$y^{(i)}$都能被$h(x^{(i)})=y^{(i)}$正确预测，那么表述为$\mathcal{H}$能够分散$S$。VC维就是，假设类$\mathcal{H}$能分散的集合$S$中,具有最大的size。具体的,如果$\mathcal{X}$是n维空间，那么对于能够分散$S$的假设类$\mathcal{H}$,$VC(\mathcal{H})=n+1$。 下面是学习理论中最重要的定理(待证明) 给定假设类$\mathcal{H}$,$VC(\mathcal{H})=d$,当误差概率至少为$1-\delta$时，对于所有假设$h$有 ||\mathcal{E}(h)-\hat{\mathcal{E}}(h)||\leq O(\sqrt{\frac{d}{m}log\frac{m}{d}+\frac{1}{m}log\frac{1}{\delta}})同时还有： \mathcal{E}(\hat{h})\leq \mathcal{E}(h^{\ast}) +O(\sqrt{\frac{d}{m}log\frac{m}{d}+\frac{1}{m}log\frac{1}{\delta}})换句话说，当一个假设类具有有限的假设维时，随着m变大，一致收敛就会发生。同时还有，当所有假设满足$|\mathcal{E}(h)-\hat{\mathcal{E}}(h)|\leq\gamma$，概率误差至少为$1-\delta$,那么$m=O_{\gamma,\delta}(d)$ 换句话说，一个好的学习过程中，训练样本的数量与假设类的VC维线性相关，而VC维又与参数个数线性相关，所以可以得到试图最小化训练误差的算法，训练集的数目与假设的参数的数目线性相关。]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
        <tag>Learning Theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Note3 SVM(2)]]></title>
    <url>%2F2018%2F07%2F23%2FCS229-Note3-SVM-2%2F</url>
    <content type="text"><![CDATA[CS229 Note3 SVM(2)这一部分的内容： 使用拉格朗日对偶解决线性可分支持向量机的最优化问题。 当样本空间线性不可分时，将原始特征空间映射到高维特征空间，再使用线性支持向量机算法求解。核函数是用于解决空间映射后维度爆炸的问题的。 当样本空间存在特异点时，引入松弛变量减少算法对特异点的敏感性。 求解拉格朗日优化问题的有效算法：SMO算法 最优边缘分类器(线性可分支持向量机)原始问题寻找最优边缘分类器的原始问题 min_{\gamma,w,b}\frac{1}{2}||w^2|| \\ s.t. \quad y^{(i)}(w^Tx^{(i)}+b) \geq 1,i=1,...,m约束条件可以被写为拉格朗日函数中的形式： g_i(w)=-y^{(i)}(w^Tx^{(i)}+b)+1\leq0考虑到KKT对偶互补条件$\alpha_ig_i(w)=0$，仅仅对于函数间距等于1的训练样本有$\alpha_i&gt;0$.在下图中，最大间距分离超平面用实线表示。 有着最小间距的点是离决策边界最近的点，在上图中只有三个点存在于与决策边界平行的虚线上，因此，只有这三个训练样本对应的$\alpha_i$在优化问题中是非0的，这三个点也叫做支持向量，这也是支持向量的数量远远小于训练样本数量的原因。 之所以列出优化问题的对偶形式，一个关键的思想是试图根据特征空间中点之间的内积形式($$)来构造算法,向量化的表示是$((x^{(i)})^Tx^{(j)})$，这正是接下来可以使用核技巧的关键。 现在根据优化问题构造拉格朗日函数： \mathcal{L}(w,b,\alpha)=\frac{1}{2}||w^2||=-\sum_{i=1}^m\alpha_i[y^{(i)}(w^Tx^{(i)}+b)-1]没有$\beta_i$这个拉格朗日乘子，是因为没有不等约束条件。 对偶问题考虑拉格朗日函数$\mathcal{L}(w,b,\alpha)$，首先将拉格朗日乘子$\alpha_i$看作常数，再对该函数进行最小化（固定$\alpha_i$后，求$w$和$b$的偏导为0） \Delta_w\mathcal{L}(w,b,\alpha)=w-\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}=0 \\ w = \sum_{i=1}^m\alpha_iy^{(i)}x^{(i)} \\ \frac{\partial}{\partial_b}\mathcal{L}(w,b,\alpha) = \sum_{i=1}^{m}\alpha_iy^{(i)}=0将$w$的值代入原始问题，通过化简可以得到 \mathcal{L}(w,b,\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}-b\sum_{i=1}^m\alpha_iy^{(i)}.上式最后一项为0，得到 \mathcal{L}(w,b,\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}现在对偶优化问题如下： max_\alpha \quad W(\alpha) = \sum_{i=1}^m\alpha_i -\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j. \\ s.t. \alpha \geq 0,i=1,...,m \\ \sum_{i=1}^m\alpha_iy^{(i)}=0由于原始问题满足拉格朗日对偶问题中“存在$w^{\ast}$是原始问题的解,$\alpha^{\ast}$,$\beta^{\ast}$是对偶问题的解”的条件，因此求解原始问题可以转化为求解对偶问题。假如我们能求解该优化问题，得到$\alpha^{\ast}$,那么w^{\ast}=\sum_{i}\alpha_i^{\ast}y_ix_i这其中至少有一个$\alpha_j^{\ast}&gt;0$,对此$j$有 y_j(w^{\ast}.x_j+b^{\ast})-1=0将$w^{\ast}$的值代入上式，得到 b^{\ast}=y_j-\sum_{i=1}^{N}\alpha_i^{\ast}y_i上式是在《统计学习方法》中的计算方法，在CS229中，$b^{\ast}$的计算方法是 b^{\ast}=-\frac{max_{i:y^{(i)}=-1}{w^{\ast}}^Tx^{(i)}+min_{i:y^{(i)}=1}{w^{\ast}}^Tx^{(i)}}{2}考虑在决策边界上，$b = -w^Tx$,因此分别假设间距最小的正样本或者负样本在决策边界上，再将他们对应的$b$取平均值，就是一个最合理的$b*$。假如我们已经拟合出$\alpha$，现在要对一个新样本进行预估，那么就要计算 \sum_{i=1}^m\alpha_iy^{(i)}+b由于只有支持向量的$\alpha_i$是非0的，因此我们其实只需要计算支持向量与样本之间的内积。接下来将核函数应用到支持向量机中，使其在高维空间中依然高效。 Kernels特征空间的映射之前所讨论的情况都是在训练样本线性可分的条件下进行的，如果训练样本如下图所示，要使用之前的线性可分支持向量机算法，就需要把当前特征空间映射到新的特征空间使训练样本线性可分。 在上图的二维空间中，决策边界可以用一个圆表示，其方程为 a_1X_1+a_2X1^2+a_3X_2+a_4X2^2+a_5X_1X_2+a_6=0观察上式的结构，可以构造一个五维的空间，五个坐标的值为$Z_1=X_1$,$Z_2=X_1^2$,$Z_3=X_2$,$Z_4=X_2^2$,$Z_5=X_1X_2$,在新坐标系下，决策边界可以写作： \sum_{i=1}^5a_iZ_i+a_6=0上面的思路就是核方法处理非线性问题的基本思想。 通过映射$\phi(.)$将样本属性映射到一个高维空间中，使数据变得线性可分。但是映射空间的维度会随着原始空间的维度指数上升，还可能会出现新空间是无穷维导致无法计算的情况(高斯核函数)。此时就需要使用Kernel 核函数假设有两个向量$x_1=(\eta_1,\eta_2)^T$和$x_2=(\xi_1,\xi_2)^T$,$\phi(.)$是之前所说的五维空间的映射。这两个向量映射后的内积为： =\eta_1\xi_1+\eta_1^2\xi_1^2+\eta_2\xi_2+\eta_2^2\xi_2^2+\eta_1\eta_2\xi_1\xi_2另外 (+1)^2=2\eta_1\xi_1+\eta_1^2\xi_1^2+2\eta_2\xi_2+\eta_2^2\xi_2^2+2\eta_1\eta_2\xi_1\xi_2+1以上两个式子具有很多相似的地方，实际上如果把原始映射$\phi(.)$某几个维度线性缩放,再加上一个常数维度，定义为$Z_1=\sqrt2 X_1$,$Z_2=X_1^2$,$Z_3=\sqrt2 X_2$,$Z_4=X_2^2$,$Z_5=X_1X_2$,$Z_6=1$。这样定义映射之后。$&lt;\phi(x_1),\phi(x_2)&gt;$的内积与上式是相等的。他们的区别在于：定义映射$\phi(.)$后，需要将映射结果放到高维空间中计算内积，而$(+1)^2$就在原来的低维空间中进行计算，不需要知道映射$\phi(.)$。所以在维度爆炸的情况下第一种方式无法计算，而第二种方法仍然能够从容处理。第一种的时间复杂度是$O(n^2)$,第二个的时间复杂度是$O(n)$,$n$为样本的属性数量。 计算两个向量在映射后空间中内积的函数叫做核函数(Kernel Function),在上面的例子中，核函数为: K(X_1,X_2)=(+1)^2在支持向量机中正好有需要计算映射后空间中的内积的地方在分类函数中： \sum_{i=1}^{n}\alpha_iy_iK(x_i,x)+b在优化对偶问题中的$\alpha$时： max_\alpha \quad W(\alpha) = \sum_{i=1}^m\alpha_i -\frac{1}{2}\sum_{i,j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_jK(x_i,x). \\ s.t. \alpha \geq 0,i=1,...,m \\ \sum_{i=1}^m\alpha_iy^{(i)}=0通过运用核函数，就避免了空间映射时在高维空间中进行计算，但是计算结果却是等价的。 常用核函数 多项式核函数$K(x_1,x_2)=(+R)^d$，映射后的空间维度是$\bigl( \begin{smallmatrix} m+d \\ d \end{smallmatrix} \bigr)$,$m$是原始空间的维度 高斯核函数$K(x_1,x_2)=exp(-\frac{||x_1-x_2||^2}{2\sigma^2})$，当$\sigma$取值大，高次特征权重衰减很快，实际上映射的是一个低维子空间，反过来$\sigma$取值小时，可以将任意数据线性可分，但是会带来过拟合问题。 字符串核函数 正定核如果一个核函数能够符合特征映射$\phi(.)$，那么它的Kernel martix K是一个半正定对称矩阵，$K_{ij}=K(x^{(i)},x^{(j)})=\phi(x^{(i)})^T\phi(x^{(j)})=\phi(x^{(j)})^T\phi(x^{(i)})=K(x^{(j)},x^{(i)})$ 线性支持向量机与软间隔最大化现实情况中，训练数据中会有一些特异点(outlier)，这些特异点去除后，剩下的样本点组成的几何是线性可分的。这些特异点的存在会使样本点线性不可分，因为他们不满足最优边缘分类器$y^{(i)}(w^Tx^{(i)}+b) \geq 1,i=1,…,m$的约束条件。因此对每个样本点引入松弛变量$\xi_i\geq0$，使函数间隔加上松弛变量大于等于1，这样约束条件变为： y_i(w.x_i+b)\geq1-\xi_i对于每个松弛变量$\xi_i$，在优化问题中支付一个代价$\xi_i$，目标函数由$\frac{1}{2}||w||^2$变为: \frac{1}{2}||w||^2+C\sum_{i=1}^{N}\xi_iC值大时，对误分类的惩罚大，该优化目标的含义有两层：即使间隔尽量大并且误分类的点的个数尽量小。对于存在特异点的样本集，这种方法称为软间隔最大化。 原始问题 min_{w,b,\xi} \quad \frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i \\ s.t. \quad y_i(w.x_i+b) \geq 1-\xi_i, i=1,..,N \\ \xi_i \geq 0 \quad i=1,2,...,N可以证明$w$的解是唯一的，但是$b$的解不唯一，存在于一个区间。 对偶问题通过构造拉格朗日函数求极大极小问题，得到对偶问题 min_\alpha \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i.x_j)-\sum_{i=1}^N\alpha_i \\ s.t. \sum_{i=1}^N \alpha_iy_i = 0 \\ 0 \leq \alpha_i \leq C, i=1,2,...,N求得最优解$\alpha^{\ast}$后,计算$w^{\ast}=\sum_{i=1}^N\alpha_iy_ix_i$，再选择一个合适的分量$\alpha_j$,计算$b^{\ast}=y_j-\sum_{i=1}^Ny_i\alpha_i^{\ast}$。最后得到分离超平面。 序列最小优化算法(SMO)支持向量机问题可以形式化为求解凸二次规划问题，具有全局最优解，但是当训练样本很大时，很多优化方法都很低效，SMO算法就是一种快速求解拉格朗日对偶问题最优解$\alpha^{\ast}$的算法。 基本思路：每次选择两个$\alpha$的分量作为变量，其余分量作为固定的量，构建一个二次规划问题，该问题关于这两个分量的解会更接近于原始问题中这两个分量的解，因为这两个分量会使对偶问题的函数值越来越小，使整体越来越接近满足KKT条件。重要的是这样分解出来的问题能够通过解析方法求解，所以可以大大提高速度，子问题有两个变量，一个是违反KKT条件最严重那个$\alpha$的分量，另一个变量由优化问题的约束条件自动确定，因此不断将该问题分解为子问题求解，可以逐步求出$\alpha$的。 SMO算法包括两部分：求解子问题的解析方法与及选择子问题合适变量的启发式方法。 求解子问题的解析方法基本思路：由于选择了两个分量$\alpha_1,\alpha_2$作为变量，可以从约束条件$\sum_{i=1}^N\alpha_iy_i=0$得到： \alpha_1y_1+\alpha_2y_2=-\sum_{i=3}^{N}y_i\alpha_i=\varsigma在以$\alpha_1,\alpha_2$为变量的平面上，上式是一条斜率为1或者-1的直线 另外，$\alpha_2$本身处于$[0,C]$区间内，加上$\alpha_2$在一条直线上，运用线性规划的值，可以进一步细分$\alpha_2$的取值范围。 现在先求解出没有对$\alpha_2$进行取值范围约束时的解(将$\alpha_1$用$\alpha_2$表示，代入目标函数子问题，对$\alpha_2$求导置为0，解得此时$\alpha_2$的值)。 最后与约束条件进行比较，确定$\alpha_2$的值,再确定$\alpha_1$的值 变量的选择方法 选择第一个变量：外层循环，首先检验支持向量是否满足是否满足KKT条件，如果都满足，再遍历整个训练集，检验他们是否满足。 选择第二个变量：内层循环，假设已经找到不满足KKT条件的$\alpha_1$，寻找第二个变量$\alpha_2$的标准是希望$\alpha_2$的有足够大的变化($\alpha_2$的变化越快，$\alpha_1$的变化也越快，也越接近于最优解)，这里$\alpha_2$的变化受$E_1-E2$影响(这是在前面求解$\alpha_2$过程中得到的)，$E_i=g(x_i)-y_i$,即预测值与真实值的差。有了以上度量$\alpha_2$变化的标准，就可以确定该选择谁作为$\alpha_2$了， 计算阈值$b$和差值$E_i$:每次优化子问题的两个变量后，都要更新$b$,对于$\alpha1和\alpha_2$,根据KKT条件，观察形式可以代入$E_1,E_2$求得$b_1^{new},b_2^{new}$，如果$\alpha_1^{new},\alpha_2^{new}$在区间(0,C),那么两个b是相等的(此时这两个点都是支持向量),如果$\alpha_1^{new},\alpha_2^{new}$为0或者C，那么两个b之间的书都符合KKT条件的阈值，此时选择它们的中点，完成变量的优化后，还要更新对应的$E_i$，避免之后重复计算。 参考内容 核函数部分 《统计学习方法》]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSS229 Note3 SVM(1)]]></title>
    <url>%2F2018%2F07%2F18%2FCS229-Note3-SVM-1%2F</url>
    <content type="text"><![CDATA[CS229 Note3 SVM(1)这篇博文主要内容是 支持向量机的引入 引入函数间距与几何间距 在几何间距基础上定义最优边缘分类器 引入拉格朗日对偶性(以解决最优边缘分类器的优化问题) 符号表示 支持向量机分类器： h_{w,b}(x)=g(w^Tx+b)if$g(z)\geq0$,$g(z)=1$,otherwise,$g(z)=-1$。 函数间距与几何间距(Functional and geometric margins)函数间距给定一个训练实例$(x^{(i)},y^{(i)})$，$(w,b)$的函数间距被定义为: \hat{\gamma^{(i)}}=y^{(i)}(w^Tx+b)如果间距大于0，代表预测是正确的，间距的值越大，代表预测正确的置信度越大。 用函数间距来度量分类器的效果有一个特性，如果同时n倍缩放$w$和$b$，由于$g(w^Tx+b)=g(2w^Tx+2b)$，这不会改变$h_{w,b}(x)$的值，其只受符号影响，不受量级影响，通过缩放$w$和$b$，函数间距会被缩放，但是不会对分类器效果有任何有意义的影响，因此用函数间距来度量分类器效果是有缺陷的。直观地看，引入某种归一化条件可能会起作用，例如$||w||_2=1$。给定一个训练集$S=\{(x^{(i)},y^{(i)});i=1,…,m\}$，训练集的函数间距是所有样本的函数间距中最小的那个。 \hat{\gamma}=min_{i=1,...,m}\hat{\gamma^{(i)}}几何间距决策超平面是$w^Tx+b=0$，那么$w$是与分割超平面垂直的。假设有A点，代表标签$y^{(i)}$=1的输入样本$x^{(i)}$，它到决策边界的距离$\gamma^{(i)}$，用线段AB表示。 如何得到$\gamma^{(i)}$的值？$w/||w||$是与$w$方向相同的单位向量，由于$A$点用$x^{(i)}$表示，$B$点可以用$x^{(i)}-\gamma^{(i)}.w/||w||$，由于$B$点在决策边界上,有： w^T(x^{(i)}-\gamma^{(i)}\frac{w}{||w||})+b=0从上式可以求得 \gamma^{(i)}=\frac{w^Tx^{(i)}+b}{||w||}=(\frac{w}{||w||})^Tx^{(i)}+\frac{b}{||w||}上式是对于正样本的情况，一般的，样本$(x^{(i)},y^{(i)})对$$(w,b)$的几何间距： \gamma^{(i)}=y^{(i)}((\frac{w}{||w||})^Tx^{(i)}+\frac{b}{||w||})如果$||w||=1$，函数间距与几何间距相等。但是几何间距不会受参数的缩放影响，这个特性在之后的推导中是及其有用的，比如我们可以缩放参数来满足某些关于$w$的约束。 最优边缘分类器给定一个训练集，如何用具有最大几何间距的分类器区分正负样本？ max_{\gamma,w,b} \quad \gamma \\ s.t.\quad y^{(i)}(w^Tx^{(i)}+b) \geq\gamma,i=1,...,m \\ ||w||=1.第一个约束条件表示所有样本的函数间距至少是$\gamma$,第二个约束条件确保函数间距与几何间距相等，所以也有几何间距至少是$\gamma$的约束条件。因此，解决该最大化问题可以求得具有最大几何间距的$(w,b)$。但是$||w||=1$时是一个非凸的问题，不能被优化软件求解，所以需要对优化问题进行变形 max_{\gamma,w,b} \quad \frac{\hat{\gamma}}{||w||} \\ s.t.\quad y^{(i)}(w^Tx^{(i)}+b) \geq\hat{\gamma},i=1,...,m现在，最大化的是$\hat{\gamma}/||w||$，约束条件是所有函数间距至少为$\hat{\gamma}$，因为几何间距和函数间距的关系是$\gamma=\hat{\gamma}/||w||$，所以这恰好是我们想要的答案。但是现有软件依旧不能解决这个优化问题，所以需要继续变形。因为对$w$和$b$进行缩放不会影响决策边界，所以通过缩放$w$和$b$添加约束：训练集的函数间距必须为1。 \hat\gamma=1于是问题就变成了最小化$||w||$，等价于 min_{\gamma,w,b} \frac{1}{2}||w^2|| \\ s.t. \quad y^{(i)}(w^Tx^{(i)}+b) \geq 1,i=1,...,m现在的问题就变得容易解决了，其结果是一个最优边缘分类器，可以使用二次规划代码来求解。现在，引入拉格朗日对偶(Lagrange duality)，它的主要内容是优化问题的对偶形式，最优边缘分类器在高维空间中能够通过核函数进行有效工作，优化问题的对偶形式有关键作用，另外对偶形式还能推导出一个比一般QP软件更为有效的算法去解决以上的优化问题。 拉格朗日对偶(Lagrange duality)考虑有以下形式的问题 min_w \quad f(w) \\ s.t. \quad h_i(w)=0,i=0,...,l.我们定义拉格朗日函数为 \mathcal{L}(w,\beta)=f(w)+\sum_{i=1}^l\beta_ih_i(w)$\beta_i$被称作拉格朗日乘子，通过将$\mathcal{L}$的偏导置为0，可以求解$w$和$\beta$。 原始优化问题这里的原始优化问题有一个不等式约束和一个等式约束 min_w \quad f(w) \\ s.t. g_i(w) \leq 0, i=1,...,k \\ h_i(w) = 0,i =1,...,l.为了解决该问题，定义拉格朗日函数为 \mathcal{L}(w,\alpha,\beta)=f(w)+\sum_{i=1}^k\alpha_i g_i(w)+\sum_{i=1}^{l}\beta_i h_i(w)$\alpha_i$与$\beta_i$是拉格朗日乘子引入如下变量 \theta_\mathcal{P}(w)=max_{\alpha,\beta;\alpha_i\geq0}\mathcal{L}(w,\alpha,\beta)$\mathcal{P}$代表”primal”(原始)，如果参数$w$违反了两个约束，易得： \theta_\mathcal{P}(w)=\infty相反，如果参数$w$满足了两个约束,$\theta_\mathcal{P}(w)=\infty$,因此 \theta_\mathcal{P}(w)= \begin{cases} f(w), & \text {if $w$ satisfies primal constraints} \\ \infty, & \text{otherwise} \end{cases}因此，当$w$满足约束时，对$\theta_\mathcal{P}(w)$求最小值，就是对$f(w)$求最小值。所以原始优化问题就可以表示为： min_w\theta_\mathcal{P}(w)=min_w \quad max_{\alpha,\beta;\alpha_i \geq 0}\mathcal{L}(w,\alpha,\beta)即，先把$w$看作常量，求 \theta_{\mathcal{P}}(w)=max_{\alpha,\beta;\alpha_i \geq0}\mathcal{L}(w,\alpha,\beta)确定$\alpha$和$\beta$后，原始优化问题的解被定义为p^{\ast}=min_w\theta_\mathcal{P}(w)即原始问题的最小最大化。 对偶优化问题原始问题的对偶问题被定义为 \theta_\mathcal{D}(\alpha,\beta)=min_w\mathcal{L}(w,\alpha,\beta)$\mathcal{D}$代表”Dual”，即把$\alpha$与$\beta$看作常量，求最小化$\theta_\mathcal{D}(\alpha,\beta)=min_w\mathcal{L}(w,\alpha,\beta)$确定$w$以后，对偶优化问题被表示为最大最小化 max_{\alpha,\beta;\alpha_i\geq0}\theta_\mathcal{D}(\alpha,\beta)=max_{\alpha,\beta;\alpha_i\geq0}\quad min_w\mathcal{L}(w,\alpha.\beta)所以对偶优化问题的解表示为 d^*=max_{\alpha,\beta;\alpha_i \geq 0}\theta_\mathcal{D}(\alpha,\beta)原始问题与对偶问题的联系d^{\ast}=max_{\alpha,\beta;\alpha_i \geq 0}\theta_\mathcal{D}(\alpha,\beta) \leq min_w\theta_\mathcal{P}(w) = p^{\ast}在特定条件下，我们有 d^{\ast}=p^{\ast}所以可以通过求解对偶优化问题来代替求解原始优化问题。假设$f$和$g_i$都是凸状的，$h_i$是仿射函数，以及存在$w$使约束$g_i(w) &lt; 0$成立。在以上条件下，一定存在$w^{\ast}$,$\alpha^{\ast}$,$\beta^{\ast}$使$w^{\ast}$是原始优化问题的最优参数，$\alpha^{\ast}$,$\beta^{\ast}$是对偶问题的最优参数，以及$p^{\ast}=d^{\ast}=\mathcal{L}(w^{\ast},\alpha^{\ast},\beta^{\ast})$,此时，$\alpha^{\ast},w^{\ast},\beta^{\ast}$满足以下KKT条件： \frac{\partial}{\partial_{w_i}}\mathcal{L}(w^\ast,\alpha^\ast,\beta^\ast)=0,i=1,...,n \\ \frac{\partial}{\partial_{\beta_i}}\mathcal{L(w^\ast,\alpha^\ast,\beta^\ast)}=0,i=1,...,l \\ a_i^{\ast}g_i(w^{\ast})=0,i=1,...,k \\ g_i(w^{\ast}) \leq 0,i=1,...,k \\ a^{\ast} \geq 0,i=1,...,k$w^{\ast}$,$\alpha^{\ast}$,$\beta^{\ast}$参数满足KKT条件与$w^{\ast}$,$\alpha^{\ast}$,$\beta^{\ast}$是原始问题和对偶问题最优参数互为充要条件。等式(3)被称为，KKT 对偶互补条件，该式表明如果$a_i^{\ast}&gt;0$,那么$g_i(w^{\ast})=0$,该条件是推导“支持向量机只有少数支持向量”的关键，该条件也对推导SMO算法有极大帮助(convergence test)。]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229-Note2]]></title>
    <url>%2F2018%2F07%2F08%2FCS229-Note2%2F</url>
    <content type="text"><![CDATA[CS229 Note2本章主要内容是生成学习算法与朴素贝叶斯模型 生成学习算法与使用决策边界进行分类的方法不同(逻辑回归与感知机算法)，生成学习算法对不同的类别分别建立分类模型，将要预测的样本与不同的模型进行匹配，观察新样本更符合哪一个类别。 试图直接学习$p(y|x)$（如逻辑回归），或者学习输入空间$X$与标签$\{0,1\}$之间的直接映射的算法叫做判别学习算法(discriminative learning algorithms)。对于对$p(x|y)$以及$p(y)$进行建模的算法，叫做生成学习算法(generative learning algorithms)，$p(x|y=1)$模拟了类别1的特征分布，$p(x|y=0)$同理。 在对$p(y)$(class priors)以及$p(x|y)$进行建模之后，使用贝叶斯法则推导给定$x$时$y$的后分布。 p(y|x)=\frac{p(x|y)p(y)}{p(x)}p(x)=p(x|y=1)p(y=1)+p(x|y=0)p(y=0)进行预测时，其实不用计算分子，因为可以认为每种输入特征发生的概率可以认为是一样的，当$p(y)$为0.5，该项相同也可以省去。 \begin{align} \mathop{\arg\max}_{y} \ \ p(y|x)&=\mathop{\arg\max}_{y}\frac{p(x|y)p(y)}{p(x)} \\ &= \mathop{\arg\max}_{y}p(x|y)p(y) \end{align}高斯判别分析(GDA)在高斯判别分析(Gaussian discriminant anlysis)中，假设$p(x|y)$遵循多元正态分布。 多元正态分布多元正态分布被一个平均向量$u \in R^n$和协方差矩阵$\Sigma \in R^{n*n} \geq 0$，这是一个对称的半正定矩阵。概率密度： p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{1/2}|\Sigma|^{-1}}exp(-\frac{1}{2}(x-\mu)^T{\Sigma}^{-1}(x-\mu)$|\Sigma|$是矩阵的行列式，平均值$u$的计算方法是期望$E[X]=\int_x {xp(x;u,\Sigma)} \,{\rm d}x=u$，对于矩阵$Z$,其协方差矩阵Cov(Z)=E[(Z-E[Z])(Z-E[Z])^T]=E[ZZ^T]-(E[Z])(E[Z])^T如果$X \thicksim N(\mu,\Sigma)$ Cov(X)=\Sigma对于多元正态分布，参数$\mu$是正态分布的中心点的坐标，协方差矩阵$\Sigma$影响正态分布的形状，具体影响见下图。 最左边的是标准的正态分布，当增加对角元素的值时，密度在45度角上产生了压缩。 当减小对角线上的值，可以看到密度再一次被压缩，不过是在相反的方向上。 高斯判别分析模型(Gaussian Discriminant Analysis model)在分类问题中，输入特征$x$是连续值随机变量，可以使用高斯判别分析进行建模，$p(x|y)$被建模为多元正态分布。 y \thicksim Bernoulli(\phi)\\x|y = 0 \thicksim N(\mu_0,\Sigma)\\ x|y = 1 \thicksim N(\mu_1,\sigma)\\ p(y)=(\phi)^y(1-\phi)^{1-y}\\ p(x|y=0) = \frac{1}{(2\pi)^{n/2}}exp(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0))\\ p(x|y=1) = \frac{1}{(2\pi)^{n/2}}exp(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1))\\模型的参数分别是$\phi$，$\Sigma$，$\mu_1$，$\mu_0$（注意平均向量$\mu_0$和$\mu_1$是不同的，协方差矩阵$\Sigma$是一样的），数据的极大似然估计如下。\begin{align}\ell(\phi,\mu_0,\mu_1,\Sigma) &amp;= log \prod_{i=1}^mp(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\&amp;= log \prod_{i=1}^mp(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)\end{align}通过最大化似函数，参数的估计如下： \phi=\frac{1}{m}\sum_{i=1}^m1\{y^{(i)}=1\} \\ \mu_0 = \frac{\sum_{i=1}^m1\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^m1\{y^{(i)}=0\}}\\ \mu_1 = \frac{\sum_{i=1}^m1\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^{m}1\{y^{(i)}=1\}}\\然后使用贝叶斯法则就能预测给定x时y的概率。从绘图上来看，算法所做的事情如下 GDA与逻辑回归如果把$p(y=1|x;\phi,\mu_0,\mu_1,\Sigma)$看作$x$的函数，可以发现其形式为 \frac{1}{1+exp(-\theta^Tx)}$\theta$可以被看做其余四个参数的函数，这恰好是逻辑回归的形式。GDA比逻辑回归具有更严格的建模假设，当$p(x|y)$确实是高斯分布，那么GDA是渐近有效(asymptotically efficient)的，也就是说在训练集非常大的条件下，没有算法比GDA更好，即使是小型的数据集，GDA的效果也比逻辑回归好。相比之下，如果对数据的假设约束更弱，如$p(x|y)$是泊松分布，或者不确定是什么分布，那么逻辑回归会更强健，对错误数据也不会特别敏感。 朴素贝叶斯如果在分类问题中输入的特征是离散的值，如垃圾邮件文本分类，首先约定一个字典 dict={a,aardvark,…,zygmurgy}和字典向量 x= \begin{bmatrix} 1 \\ ... \\ 1 \\ ... \\ 0 \end{bmatrix}字典向量中第$i$为1，则代表字典中序号为$i$的单词出现在了这封邮件中。有了特征向量，接下来对$p(x|y)$进行建模，如果字典中有50000个单词，直接用多元分布对$p(x|y)$进行建模,特征会有$2^50000$种可能。那么参数向量的维度会有$2^50000-1$，实在是太多了。为了对$p(x|y)$进行建模，我们对数据加了一个较强的约束假设，即$x_i$对于给定条件$y$是独立的，即$y$发生的条件下，$x_i$之间互不影响。这个假设叫做朴素贝叶斯假设(Naive Bayes assumption)，得到的算法是朴素贝叶斯分类器(Naive Bayes classifier)。比如，在一个字典中，”buy”是第2087个单词，”price”是第39381个单词，那么如果一封邮件是垃圾邮件，那么已知其中一个单词出现了，不会影响另一个单词出现的概率。于我们就有p(x\_{2087}|y)=p(x\_{2087}|y,x\_{39831}) \begin{align} p(x_1,...,x\_{5000}) &= p(x_1|y)p(x_2|y,x_1)p(x3|y,x_1,x_2)...p(x_{50000}|y,x_1...,x_{49999})\\ &=p(x_1|y)p(x_2|y)p(x_3|y)...p(x_50000|y)\\ &=\prod_{i=1}^np(x_i|y) \end{align}即便朴素贝叶斯假设是一个约束非常强的假设，朴素贝叶斯分类器依然可以在很多问题上很好的工作。 模型的参数化： \phi_{i|y=1}=p(x_i=1|y=1)\\ \phi_{i|y=0}=p(x_i=1|y=0)给定训练集$\{(x^{(i)},y^{(i)};i=1,…,m)\}$数据的联合似然函数为： \mathcal{L}(\phi_y,\phi_{i|y=0},\phi_{i|y=1})=\prod_{i=1}^mp(x^{(i)},y^{(i)})对该似然函数进行最大似然估计： \phi_{j|y=1}=\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=1\}}{\sum_{i=1}^m1\{y^{(i)}=1\}}\\ \phi_{j|y=0}=\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}}\\ \phi_y = \frac{\sum_{i=1}^m1\{y^{^{(i)}}=1\}}{m}拟合了所有参数之后，预测一个新样本的计算方法为： \begin{align} p(y=1|x)&=\frac{p(x|y=1)p(y=1)}{p(x)}\\ &=\frac{(\prod_{i=1}^np(x_i|y=1))p(y=1)}{(\prod_{i=1}^np(x_i|y=1))p(y=1)+(\prod_{i=1}^np(x_i|y=0))p(y=0)} \end{align}该算法中的参数$x_i$不一定是binary-value，可以推广为一般离散的值，如{1,2,…,k}也可行，例如对于连续的特征使用多元正态分布不能很好建模时，对特征进行离散化再使用朴素贝叶斯通常会有更好的表现。 拉普拉斯平滑(Laplace smoothing)假如在预测一封邮件时，出现了没在以前的邮件里出现过的单词$x_{35000}$。那么$\phi_{35000}|y=1$与$\phi_{35000}|y=0$的值都是0，无法对$p(y=1|x)$进行预测。将这个问题推广开来，将训练集中未出现的时间概率估计为0是一个坏主意，估计一个多元随机变量z在{1,…,k}中取值，有$\phi_i=p(z=i)$,给定观察值${z^{(1)},…,z^{(m)}}$，最大似然估计为 \phi_j=\frac{\sum_{i=1}^m\{z^{(i)}=j\}}{m}要解决之前预测未出现过的单词的问题，使用拉普拉斯平滑 \phi_j=\frac{\sum_{i=1}^m\{z^{(i)}=j\}+1}{m+k}然后就可以对有未出现过的单词的邮件进行预测了 文本分类的事件模型首先引入不同的符号和特征来代表邮件，$x_i$表示邮件中的第$i$个单词，从{1,…,|V|}中取值，|V|是字典的大小。一封邮件被$(x_1,….,x_n)$表示，n随着不同文档变化。我们假设邮件的生成是一个随机过程，在这个过程中，是否为垃圾邮件首先被确定(根据p(y))，然后邮件发送者首先通过多元分布生成$x_1$，概率为$p(x_1|y)$，再独立与$x_1$使用相同的多元分布生成$x_2$，之后的单词都是如此。然后总体概率$p(y)\prod_{i=1}^np(x_i|y)$如果给定训练集$\{(x^{(i)},y^{(i)});i=1,…,m\}$,$x^{(i)}=(x_1^{(i)},x_2^{(i)},…,x_{n_i}^{(i)})$,$n_i$是第i个训练集的单词数。似然函数是： \begin{align} \mathcal{L}(\phi_y,\phi_{i|y=0},\phi_{i|y=1})&=\prod_{i=1}^mp(x^{(i)},y^{(i)})\\ &=\prod_{i=1}^m(\prod_{j=1}^{n_i}p()) \end{align}对参数的最大似然估计是： \phi_{k|y=1}=\frac{\sum_{i=1}^m\sum_{j=1}^{n_i}\{x_J^{(i)}=k\wedge y^{(i)}=1\}}{\sum_{i=1}^m1\{y^{(i)}=1\}n_i}分子是垃圾邮件的单词数之和，分母是所有垃圾邮件中存在字典中第k个单词的的单词数之和。 \phi_{k|y=1}=\frac{\sum_{i=1}^m\sum_{j=1}^{n_i}\{x_J^{(i)}=k\wedge y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}n_i}分子是非垃圾邮件的单词数之和，分母是所有非垃圾邮件中存在字典中第k个单词的的单词数之和。 \phi_y=\frac{\sum_{i=1}^m1\{y^{(i)}=1\}}{m}拉普拉斯平滑 \phi_{k|y=1}=\frac{\sum_{i=1}^m\sum_{j=1}^{n_i}\{x_J^{(i)}=k\wedge y^{(i)}=1\}+1}{\sum_{i=1}^m1\{y^{(i)}=1\}n_i+|V|} \phi_{k|y=0}=\frac{\sum_{i=1}^m\sum_{j=1}^{n_i}\{x_J^{(i)}=k\wedge y^{(i)}=0\}+1}{\sum_{i=1}^m1\{y^{(i)}=0\}n_i+|V|}然后使用贝叶斯公式进行预测 \begin{align} p(y=1|x)&=\frac{p(x|y=1)p(y=1)}{p(x)}\\ &=\frac{(\prod_{i=1}^np(x_i|y=1))p(y=1)}{(\prod_{i=1}^np(x_i|y=1))p(y=1)+(\prod_{i=1}^np(x_i|y=0))p(y=0)} \end{align}]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[try to try again]]></title>
    <url>%2F2018%2F06%2F20%2Ftry-to-try-again%2F</url>
    <content type="text"><![CDATA[Try to try again 菜是原罪高中时代开着who is your daddy的秘籍看war3剧情或者打dota1只为了看人物放技能时，我觉得稍微练一下，我也可以在一块小小的地图上体验竞技的乐趣，然而依赖秘籍的我在匹配时终于体会到了什么叫做“菜是原罪”，现在即将毕业的我对Dota已经是弃坑状态了，虽然dota1已经变成了Dota2，版本也更迭也让我成为了云玩家（我好像一直都是云玩家），但是每逢比赛我还是会看，因为无论是个人秀还是完美团，都十分具有观赏性，这正是这个游戏吸引我的地方，也是我不能融入这个游戏的地方，简单的说就是我太菜，个人秀不起来也团队也配合不来。 我不是一个游戏热爱者，玩过的游戏很少，也不知道我处于游戏鄙视链的哪一层，我应该是非游戏玩家。令我印象深刻的游戏有高中时代接触到的《机械迷城》，《植物精灵》，还有第一个真正意义上从开始玩结束的游戏，《植物大战僵尸》，我喜欢这个游戏，喜欢不同的植物、僵尸相辅相成相生相克的原理，以及图鉴中每个角色的小故事，也许喜欢这种类型的游戏也是我入坑阴阳师五百多天的原因，同样的是风格独特的式神造型，式神、御魂的配合、克制，以及大部分都很悲伤的式神传记。 大学时期我玩过的游戏仍然非常少（并不代表我热爱学习，现在回想才发现自己好无趣啊哈哈），阴阳师是一个，但我只是一个咸鱼签到玩家。另外室友推荐的《艾迪芬奇的记忆》，《看火人》，《伊森卡特的消失》，以及我自己找的《奇异人生》，这类冒险游戏也有很大的魅力，它们都使用游戏这个载体讲述了一个治愈的故事，让玩家通过独特的交互方式去寻找一个完整的故事线索。 Keep Calm And Carry On在上面的游戏中，最令我感慨的是《奇异人生》。“如果我掌控了时间，是不是就能改变一切呢？”这个疑问吸引了我。 在整个游戏过程中，我的原则是“改变一切遗憾”。我尽量避免了我认为所有不好的事件发生，并将世界线的发展往我喜欢的方向引导。向不平之事伸出援手，改写挚友的人生轨迹，向喜欢的人表达心意。然而这个世界是混沌的，作为个体，一点小小的偏差就会引起巨大的变动，虽然在决定论中，变动已经是整体的发展，可以被统计的稳定性抵消，但是未来很大程度上仍然取决于现在，而我们只是整体混沌系统中的个体，任何我所认为合理的变动都会引起难以预料的发展轨迹。我改写挚友的人生轨迹，弥补了遗憾却造成了更大的伤害，而且世界线看起来依然在向最初收束，无论我怎样避免，它都会往最初的结果收敛，而我一次又一次的**try to try again**根本不能让所有事情如愿，只会出现越来越多的混乱。在结局出现以前，我依然天真地认为依靠我的能力，一切都会按照我所设想的好起来，当然最终我还是惨遭现实毒打。 也许就是这样，即便拥有超能力，依然会有无法挽回的事情。我们作出抉择的那一天，在日记上相当沉闷平凡，或许一场巨变就此发生，但是从不同的时间跨度来看，没有完全好的或者坏的选择，**keep calm and carry on**才是我们唯一可以决定的东西。 Chaos最后这段文字是布丰有关混沌(CHAOS)的描述。 世界运转无止息因为万物都在时间的洪流中相遇在广袤无尽的空间中以及物换星移的接轨瞬间万物混成冥和不拘任何形体不拘任何被赋予的形象因此，万事万物或相近 或远离或合一 或分离或相容 或互斥或生 或灭恒久不变的只有交互作用的力恣意横行，却灵巧而不自戕为宇宙燃起生命气息让生命舞台上，无时无刻上演着新的戏码写下生生不息，永无止尽的诗篇]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Note1]]></title>
    <url>%2F2018%2F06%2F17%2FCS229-Note1%2F</url>
    <content type="text"><![CDATA[CS229 Note1 只要是人都是依靠自己的知识与认知并且被之束缚生活着的，那就叫做现实。但是知识与认知是模糊不清的东西，现实也许只是镜中花水中月，人都是活在自己的执念中的。 这篇文章的主要内容包括梯度下降算法的数学原理，逻辑回归的推导，牛顿法的应用，如何构造一个广义线性模型。 梯度下降算法的数学原理算法公式 \theta=\theta_0-\eta.\Delta f(\theta_0)一阶泰勒展开f(\theta) \approx f(\theta_ 0)+(\theta-\theta_0).\Delta f(\theta_0)数学思想：曲线函数的线性拟合近似 $\theta-\theta_0$是微小矢量，其大小是步进长度$\eta$，为标量。$\theta-\theta_0$的单位向量用$v$表示。则 \theta-\theta_0=\eta vf(\theta)\approx f(\theta_0)+\eta v.\Delta f(\theta_0)我们希望每次$\theta$更新让$f(\theta)$变小。也就是说希望$f(\theta) &lt; f(\theta_0)$ f(\theta)-f(\theta_0)\approx \eta v.\Delta f(\theta_0) < 0$\eta$是标量 v.\Delta f(\theta_0) = ||\Delta f(\theta_0)||cos(\alpha)< 0其中$v$是$\theta-\theta_0$的单位向量,其方向就是梯度下降算法中要前进的方向，从上式可以看出算法要前进的方向与当前梯度方向(当前梯度方向即当前是在上升还是在下降)完全相反时，即夹角$\alpha$为180度时，$f(\theta)$减小得最快。 v.\Delta f(\theta_0) = ||\Delta f(\theta_0)||cos(\pi)v = -\frac{\Delta f(\theta_0)}{||\Delta f(\theta_0)||}因为||\Delta f(\theta_0)||是标量，可以并入$\eta$。所以 \theta = \theta_0-\eta \Delta f(\theta_0)局部加权线性回归最小化 \sum_i w^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2w^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2\tau^2})上式中参数$x$为新预测的样本特征数据，它是一个向量，参数控制了权值变化的速率离$x$很近的样本，权值接近于1，而对于离$x$很远的样本，此时权值接近于0，这样就是在局部构成线性回归，它依赖的也只是周边的点 对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数$\theta$)，没有固定的参数，是非参数算法 逻辑回归假设函数h_{\theta}(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}g(z)=\frac{1}{1+e^{-z}}该函数的实用属性： \begin{align} g'(z)&=\frac{d}{dz}\frac{1}{1+e^{-z}}\\ &=\frac{1}{(1+e^{-z})^2}(e^{-z})\\ &=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})\\ &=g(z)(1-g(z)) \end{align}拟合参数概率假设 p(y|x;\theta)=(h_{\theta}(x))^y(1-h_{\theta}(x))^{1-y}对于所有训练集，最大似然估计 \begin{align} L(\theta) &= p(\overrightarrow{y}|X;\theta)\\ &=\prod_{i=1}^m p(y^{(i)}|x^{(i)};\theta)\\ &=\prod_{i=1}^m (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{align}对似然函数取log进行最大化 \ell(\theta)=log(L(\theta))=\sum_{i=1}^{m}y^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))使用梯度上升 \begin{align} \frac{\delta}{\delta\theta_j}\ell(\theta)&=(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)})\\ &=(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)})g(\theta^Tx)(1-g(\theta^Tx))\frac{\delta}{\delta\theta_j}\theta^Tx\\ &=(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j\\ &=(y-h_\theta(x))x_j \end{align}\theta_j:=\theta_j + \alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}牛顿法求函数的零点 \theta:=\theta-\frac{f(\theta)}{f'(\theta)}当我们想最大化似然函数$\ell(\theta)$(如逻辑回归)，即求$\ell’(\theta)$的零点。所以 \theta:=\theta-\frac{\ell'(\theta)}{\ell''(\theta)}在逻辑回归里，$\theta$是一个向量。 牛顿法在多维环境下的泛化为： \theta:=\theta-H^{-1}\Delta \ell(\theta)H_{ij}=\frac{\delta^2\ell(\theta)}{\delta\theta_i\delta\theta_j}其中，$\Delta\ell(\theta)$是$\ell(\theta)$在$\theta_i$维的偏导数的向量，H是$\ell(\theta)$对每一维求二阶导数的矩阵。 广义线性模型指数分布族p(y;\eta)=b(y)exp(\eta^TT(y)-a(\eta))$\eta$是自然参数(natural parameter)$T(y)$是充分统计量(sufficient statistic)伯努利分布和高斯分布都是指数分布族的特例 伯努利分布\begin{align}p(y;\phi)&amp;=\phi^y(1-\phi)^{1-y}\\&amp;=exp(ylog\phi+(1-y)log(1-\phi))\\&amp;=exp((log(\frac{\phi}{1-\phi}))y+log(1-\phi))\end{align} \eta=log(\frac{\phi}{1-\phi})T(y)=ya(\eta)=-log(1-\phi)=log(1+e^\eta)b(y)=1高斯分布\begin{align}p(y;u)&amp;=\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}(y-u)^2)\\&amp;=\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}y^2).exp(uy-\frac{1}{2}u^2)\end{align}注:$\sigma$的值不影响最终的参数收敛，所以这里取1方便理解 \eta=uT(y)=ya(\eta)=\frac{u^2}{2}=\frac{\eta^2}{2}b(y)=(\frac{1}{\sqrt{2\pi}})exp(-y^2/2)构造广义线性模型对于通过f(x)来预测随机变量y的值的问题（分类或者回归），要构造广义线性模型，y给定x的条件分布要满足以下三个条件。 $y|x;\theta $~ExponentialFamily$(\eta)$,给定$x$和$\theta$，$y$的分布遵从指数族分布，自然参数为$\eta$ 给定$x$，目标是预测$T(y)$，在大多数例子里$T(y)=y$，这意味着$h(x)=E[y|x]$（预测$h(x)$是x条件下y发生的期望） 自然参数$\eta$与输入$x$是线性相关的：$\eta=\theta^Tx$第三个条件通常被认为是“构造GLM的选择”而不是“构造GLM的假设”(PS：我也没从讲义里看到这个的推导) 普通最小二乘(线性回归)给定x，y服从高斯分布$N(u,\sigma^2)$，前面已证明高斯分布是指数分布族的特例，满足了广义线性模型第一个条件。有$u=\eta$ h_\theta(x)=E(y|x;\theta)=u=\eta=\theta^T x 第一个等式满足了广义线性模型的第二个条件。 第二个等式是高斯分布的特性，第三个等式是之前(高斯分布是指数分布族)推导出来的，第四个等式满足第三个条件。 逻辑回归给定x，y服从伯努利分布，$\phi=\frac{1}{1+e^{-\eta}}$,$E[y|x;\theta]=\phi$\begin{align}h_\theta(x) &amp;= E[y|x;\theta]\\&amp;=\phi \\&amp;= \frac{1}{1+e^{-\eta}}\\&amp;= \frac{1}{1+e^{-\theta^Tx}}\end{align}所以，逻辑回归sigmod函数的形式是广义线性模型与指数分布族的定义的结果。 Softmax Regression多分类问题，response variable仍然是离散的，但是不止两个，首先推导这个分布属于指数分布族。使用k个参数$\phi_1,…,\phi_k$来指定每一个输出的概率。实际上只需要k-1个参数，因为他们的概率都是相互独立的且和为1. \phi_k=1-\sum_{i=1}^{k-1}\phi_i定义$T(y)$为$T(y)\in R^{k-1}$,只有$T[y]_i=1$，其余元素都为0($i$代表$T(y)$属于哪一类的序号).引入符号$1\lbrace True\rbrace =1,1\lbrace False\rbrace =0$所以$T(y)$与$y$之间的关系可以描述为 (T(y))_i=1\lbrace y=i \rbrace即$T(y)$属于第$i$类时（即第$i$个元素为1），$i$与$y$相等现在开始推导该分布属于指数分布族 \begin{align} p(y;\phi) &= \phi_i^{1\lbrace y=1 \rbrace}\phi_2^{1\lbrace y=2 \rbrace}...\phi_k^1{\lbrace y=k \rbrace}\\ &=\phi_1^{(T(y))_1}...\phi_k^{1-\sum_{i=1}^{k-1}(T(y))_i}\\ &=exp((T(y))_1log(\phi_1)+(T(y))_2log(\phi_2)+...+(1-\sum_{i=1}^{k-1}(T(y))_i)+log(\phi_k))\\ &=exp((T(y))_1log(\phi_1/\phi_k)+...+(T(y))_{k-1}log(\phi_{k-1}{\phi_k})+log(\phi_k))\\ &=b(y)exp(\eta^T T(y)-a\eta) \end{align} \eta = \begin{bmatrix} log(\phi_1/\phi_k) \\ ... \\ log(\phi_{k-1}/\phi_k) \\ \end{bmatrix}a(\eta)=-log(\phi_k)b(y)=1link function \eta_i=log\frac{\phi_i}{\phi_k}e^{\eta_i} = \frac{\phi_i}{\phi_k}\phi_k\sum_{i=1}^ke^{n_i}=\sum_{i=1}^k\phi_i=1response(softmax) function \phi_i=\frac{e^{\eta_i}}{\sum_{j=1}^ke^{\eta_j}}=\frac{e^{\theta_i^{T}x}}{\sum_{j=1}^{k}e^{\theta_j^Tx}}=p(y=i|x;\theta)参数拟合 \begin{align} \ell(\theta) &= \sum_{i=1}^{m}logp(y^{(i)}|x^{(i)};\theta) &= \sum_{i=1}^{m}log\prod_{i=1}^k(\frac{e^{\theta_l^Tx^{(i)}}}{\sum_{j=1}^{k}e^{\theta_l^{T}x^{(i)}}})^{1\lbrace y^{(i)}=l \rbrace} \end{align}然后使用梯度上升或者牛顿法来解决。]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小二乘损失函数的概率解释]]></title>
    <url>%2F2018%2F06%2F02%2F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%A6%82%E7%8E%87%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[最小二乘损失函数的概率解释概率论与统计学基础求“概率”时，已知模型和参数，预测结果。进行“统计”时，已知数据，推测模型与参数(明天要答辩了，希望不要被锤)。 先验概率(Apriori)Apriori，拉丁文意为“来自先前的东西”，即在未考虑“观测数据”之前，根据以往经验和分析得到的概率，如“抛均匀硬币时正面向上的概率为0.5”。 条件概率在样本空间$\Omega$中的两个事件A、B，那么在事件B发生的条件下，事件A发生的概率是 P(A|B)=\frac{P(A\cap B)}{P(B)}后验概率基于观测数据（经验）$X$，修正原来的先验概率后所获得的更接近实际情况的概率估计。是给定证据$X$后参数$\theta$的概率p(\theta|X)，如“已知不透明袋子里的黑白球数目，连续两次不放回取出小球，知道第二次取出了白球，求第一次取出白球的概率”，此时证据$X$为“第二次取出白球”，参数$\theta$为“第一次取出白球”。后验概率是一种条件概率，但条件概率不一定是后验概率。 似然函数对于观测数据$X$，在参数集合$\theta$上的似然，就是在给定参数值的基础上，观测到某一个结果的可能性L(\theta|x)=p(x|\theta)，也就是说似然是是关于参数的函数，对于参数给定的条件，观察到的$X$的值的条件分布(我认为参数给定并不是指参数不变，似然中不变的应是观察到的证据$X$,而似然函数就是用可能的$\theta$参数去求证据$X$的概率分布)。 似然函数的重要性不是它的具体取值，而是当参数变化时似然函数的值在如何变化。 考虑连续两次抛掷一枚硬币，每次抛掷正面向上的概率都为$\theta$,若观测到抛掷结果为正正，则其似然函数可以定义为$\theta^2$，此时$\theta$的值越大越符合我们观测到的$X$(似然性越大)，即两次都正面向上。若观测到抛掷结果为正反，则其似然函数可以定义为$\theta(1-\theta)$，此时$\theta=0.5$时似然函数值最大，最符合我们观测到的结果。 概率与似然的区别似然函数在统计推测中发挥重要的作用，因为它是关于统计参数的函数，所以可以用来评估一组统计的参数，也就是说在一组统计方案的参数中，可以用似然函数做筛选。在非正式的语境下，“似然”会和“概率”混着用；但是严格区分的话，在统计上，二者是有不同。 概率是给定参数值$\theta$的情况下观察值$X$的函数,似然是给定观察值$X$时，描述参数$\theta$的情况。 贝叶斯定理(Bayes’s Theorem)根据条件概率： p(A|B) = \frac{P(A \cap B)}{P(B)}p(B|A) = \frac{P(A \cap B)}{P(A)}整理这两个方程式，可以得到乘法公式: P(A|B)P(B) = p(B|A)P(A)两边同时除以$P(A)$: P(B|A) = \frac{p(A|B)P(B)}{P(A)}将这个式子进一步推广：随机试验的样本空间为$\Omega$,$A\subset\Omega$,$B_i(i=1,2,…,n)$是$\Omega$的一个有限划分，$P(A)&gt;0,P(B_i)&gt;0$。 P(B_i|A)=\frac{P(AB_i)}{P(A)}=\frac{P(B_i)P(A|B_i)}{P(A)}由全概率公式P(B_i|A)=\frac{P(B_i)P(A|B_i)}{\sum_{j=1}^n P(B_j)P(A|B_j)} 后验概率与似然函数的关系后验概率是基于观测到的证据$X$，参数$\theta$的概率：$p(\theta|X)$。似然函数是给定参数$\theta$的集合，证据$X$的概率分布:$p(X|\theta)$。 后验概率：“已知不透明袋子里的黑白球数目，连续两次不放回取出小球，知道第二次取出了白球，求第一次取出白球的概率”，此时证据为“第二次取出白球”，参数为“第一次取出白球”。 似然函数：“已知不透明袋子里的黑白球数目，连续两次不放回取出小球，知道第二次取出了白球，给定参数集合{第一次取出白球，第一次取出黑球}，求在该集合上的似然”，此时证据$X$为“第二次取出白球”，求似然即求在不同的参数下，结果(证据)$X$出现的可能性，这个可能性不代表出现的概率，事实上，一个似然函数乘以一个正的常数之后仍然是似然函数。 贝叶斯推断与后验分布这部分内容引用自《概率导论(Dimitri P.Bertsekas)》假定我们知道$\Theta$和$X$的联合分布，其中$\Theta$是感兴趣的未知变量，$X$是观察到的随机变量的值，目标即基于X提取$\Theta$的信息，这是不是和后验概率很像呢，事实上，贝叶斯推断问题的答案就由$\Theta$的后验分布来决定。假定我们已知： 先验分布$P(\Theta)$ 条件分布$P(X|\Theta)$ 则我们得到X的观测值后，就可以运用贝叶斯法则计算后验分布列。 p\Theta|X(\theta|x)=\frac{p\Theta(\theta)pX|\Theta(x|\theta)}{\sum_{\theta^{'}p\Theta(\theta^{'})pX|\Theta(x|\theta^{'})}}最小二乘损失函数的概率解释前面写了这么多，其实我在看CS229的讲义时发现里面的相关定义在我脑海里已经成了一团浆糊，于是稍微回顾了一下，现在回到正轨来看最初疑惑的问题。 目标变量和输入变量的关系： y^{(i)}=\theta^{T}x^{(i)}+\epsilon^{(i)}$\epsilon^{(i)}$是未建模影响或者随机噪声的误差项。同时$\epsilon^{(i)}$是独立同分布的，并且满足均值为0方差为$\sigma^2$的高斯分布。 p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})\epsilon^{(i)}=y^{(i)}-\theta^Tx^{(i)}p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^(i)-\theta^Tx^{(i)})^2}{2\sigma^2})这样书写公式后，如果$\theta$和$x^{(i)}$已知，则上式就是$y^{(i)}$的分布了，它的值越大，代表这个$\theta$描述目标变量与输入变量越准确。给定$X$和$\theta$,数据的概率(这里是原文，我想应该是指$y^{(i)}$的分布)为$p(\overrightarrow y|X;\theta)$但当我们把上式看作$\theta$的函数时，我们称之为似然函数。 L(\theta)=L(\theta;X,\overrightarrow y)=p(\overrightarrow y|X;\theta)注意到$\epsilon^{(i)}$的独立假设，上式可以书写为 \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^(i)-\theta^Tx^{(i)})^2}{2\sigma^2})最大似然估计就是要将似然函数最大化。对该式取自然数为底数的对数。 \sum_{i=1}^{m}log\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^(i)-\theta^Tx^{(i)})^2}{2\sigma^2})得到 mlog\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma^2}.\frac{1}{2}\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2所以，问题就变成了最小化 \frac{1}{2}\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2,这时的$\theta$能够最好的描述输入变量与输出变量的关系。 参考文献 先验概率 后验概率 似然函数 贝叶斯定理]]></content>
      <tags>
        <tag>CS229</tag>
        <tag>machine learning</tag>
        <tag>概率论基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结：Coursera:Machine Learning By Andrew Ng]]></title>
    <url>%2F2018%2F05%2F23%2F%E6%80%BB%E7%BB%93%EF%BC%9ACoursera-Machine-Learning-By-Andrew-Ng%2F</url>
    <content type="text"><![CDATA[总结：Coursera:Machine Learning By Andrew Ng 能够和贫僧打成平手的，世上没有几人 存在的问题这门课程的概念的阐述与课后作业编程作业都力求简单明了，可以感受到省略了很多数学证明，虽然学完每一章都带着一些疑问，但正是复杂的数学原理与直观理解之间的平衡，才可以让缺乏相关基础的人感受到机器学习的魅力。机器学习的魅力感受到了，但存在的疑惑也是需要寻求答案的，下面先罗列一下我在课程中遇到的疑惑： 线性回归代价函数的形式，为什么是预测结果与训练集之间的差的平方 逻辑回归的假设为什么是S型函数 逻辑回归的代价函数的形式的由来 反向传播算法的证明 神经网络激活函数的由来与选择 主成分分析的原理 支持向量机的引入，在逻辑回归的基础上进行修改的，但是为什么要那么修改呢 为什么高斯分布可以用于异常检测 以上就是我在学习这门过程中存在的疑惑，但是肯定的，这个过程中必定存在我没有意识到，但是我仍然不清楚的地方，我也十分期待在接下来我能够有幸发现它们，接下来我也会带着这些问题继续寻求答案。 未来的计划未来计划我主要参考了知乎用户”微调”的文章如何用3个月零基础入门机器学习 需要准备的基础数学知识 线性代数：矩阵/张量乘法、求逆，奇异值分解/特征值分解，行列式，范数等 统计与概率：概率分布，独立性与贝叶斯，最大似然(MLE)和最大后验估计(MAP)等 优化：线性优化，非线性优化(凸优化/非凸优化)以及其衍生的求解方法如梯度下降、牛顿法、基因算法和模拟退火等 微积分：偏微分，链式法则，矩阵求导等 信息论、数值理论等 入门(3-6个月) Coursera:Machine Learning By Andrew Ng：入门课程，升级版本是Ng在斯坦福的CS229 Python机器学习：快速了解sklearn,内容涉及数据预处理、维度压缩、核函数、评估方法、集成学习、情感分析、聚类、神经网络、Theano Introduction to Statistical Learning with R,频率学派圣经ESL的入门版本，除去了复杂 的数学推导，加入R编程方便快速上手。 周志华《机器学习》，入门阶段的参考书，辅助网课和书籍学习。 进阶（3-6个月） Kaggle 挑战赛/练习，类似的还有国内的平台，天池竞赛以及其他公司举办的比赛，使用Kaggle将技能落到实处，避免产生什么都懂的幻觉。 sklearn文档学习，与使用kaggle的目的一致，该文档写得和教程差不多。 周志华机器学习，在kaggle挑战和阅读Sklearn文档的过程中你还会时不时的遇到一些新的名词，比如流形学习(manifold learning)等。这个时候你会发现西瓜书真的是一本中级阶段大而全的书籍:)深度学习（3-6个月） Andrew Ng深度学习课程 Deep Learning - by IanGoodFellow：阅读建议，为了补充基础可以阅读第1-5章其中也包含了一些数学知识，只关注主流神经网络知识可以阅读6-10章，介绍了DNN/CNN/RNN，需要进一步了解一些调参和应用技巧，推荐阅读11和12章。第13-20章为进阶章节，在入门阶段没有必要阅读。其实比较实际的做法是吴恩达的课程讲到什么概念，你到这本书里面可以阅读一些深入的理论进行概念加深，按章节阅读还是比较耗时耗力的。 深入研究如果你希望继续深入的话，中文可以继续阅读周志华老师的《机器学习》和李航老师的《统计学习基础》，英文可以入手《Elements of Statistical Learning》。在这个阶段，重点要形成成体系的知识脉络，切记贪多嚼不烂，切记！从阅读论文角度来说，订阅Arxiv，关注机器学习的顶级会议，如ICML/NIPS等，相关的方法在知乎上可以很容易搜索到，不在此赘述。 实践经验： 嗑盐 企业实习，打破幻想，了解工业界的主流模型。补上学术界忽视的内容，比如可视化和数据清洗。了解技术商业化中的取舍，培养大局观。 在工作中使用]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>学习总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Large Scale Machine Learning & OCR]]></title>
    <url>%2F2018%2F05%2F23%2FLarge-Scale-Machine-Learning-OCR%2F</url>
    <content type="text"><![CDATA[大规模机器学习 这是Machine learning最后两周的内容 大型数据集的学习首先检查大规模的训练集是否有必要，通过绘制学习曲线来帮助判断。 随机梯度下降算法 ![随机梯度下降](http://a2.qpic.cn/psb?/V12UJCGv0BAHHA/K*Ie3GA19xulKVb6FAsiDRvyAjrt4HrCCZqQn45FV*E!/b/dDEBAAAAAAAA&ek=1&kp=1&pt=0&bo=UgG6AFIBugABACc!&tl=1&vuin=1405591197&tm=1527062400&sce=50-1-1&rf=viewer_311 "随机梯度下降") 随机梯度下降算法在计算一个样本的代价函数倒数之后就更新参数$\theta$，不需要将所有训练集求和，还没有完成一次迭代时，随机梯度下降算法就已经走出了很远，存在的问题是每一步不一定是正确的方向，算法会逐渐走向全局最小的位置，但可能在最小值附近徘徊。 ## 小批量梯度下降 介于批量梯度下降和随机梯度下降算法之间，每计算b次训练实例更新一次参数$\theta$。 好处:可以用向量化的方式来循环b个实例。支持并行处理。 随机梯度下降算法的收敛每X次迭代之后，在更新$\theta$之前求出X次的训练实例计算代价的平均值。然后绘制这些平均值与X次迭代次数之间的函数图像。 在线学习对数据流非离线的静态数据集的学习，与随机梯度下降算法类似，对单一的实例进行学习而不是对提前定义的训练集进行循环。 ![在线学习](http://a1.qpic.cn/psb?/V12UJCGv0BAHHA/ZJF0AfMOSUCyRYUyGXk3eszzBKbQtutNUNykCg2qBTI!/b/dPQAAAAAAAAA&ek=1&kp=1&pt=0&bo=nQFyAJ0BcgABACc!&tl=1&vuin=1405591197&tm=1527062400&sce=50-1-1&rf=viewer_311 "在线学习") 对一个数据进行学习之后就可以丢弃该数据。好处是可以很好适应用户倾向，算法针对用户的当前行为不断更新模型以适应该用户。与随机梯度下降算法类似，其区别是不会使用一个固定的数据集。优点是可以可以对变化中的用户行为进行很好的预测。 Map Reduce and Data Parallelism将数据分配给多台计算机，让计算机处理数据集的一个子集，再将结果汇总求和。如果算法能够表述为”对训练结果汇总求和”，就能通过映射化简进行加速处理。 应用实例：图片文字识别问题描述和流程图(Problem Description and Pipeline)图像文字识别 文字侦测(Text detection)-区别文字与环境 字符切分(Character segmentation)-将文字分割成单一字符 字符分类(Character classification)-确定每个字符是什么 滑动窗口文字侦测：从图像中抽取对象，按不同的窗口大小来循环截取图片，缩放至模型所需大小，交给模型判断。在文字识别中，将识别出的区域进行合并扩展，通过高宽比进行过滤。 字符切分：滑动窗口抽取对象。 字符分类：根据切分出来的字符，利用神经网络、支持向量或者逻辑回归算法进行分类。 获得大量数据和人工数据从零开始创造例如字体包配上随机背景图片 处理已有数据利用已有数据进行扭曲、旋转、模糊处理 众包找人打标签 上限分析在上限分析中，选取一部分，手工提取100%正确的输出结果，看应用整体效果提升了多少。提升效果多的最需要投入更多的时间的精力。]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anomaly Detection]]></title>
    <url>%2F2018%2F05%2F08%2FAnomaly-Detection%2F</url>
    <content type="text"><![CDATA[异常检测(Anomaly Detection) Andrew Ng老师的机器学习入门基础课程算是学完了，所谓”学而不思则惘，思而不学则殆”·，复盘一下学习情况和理解程度还是很有必要的，不过要交毕业论文啦，肝完毕设再来更博。 动机(Motivation)异常检测主要应用于非监督学习问题，但是却类似于监督学习问题。给定数据集 ，我们假使数据集是正常的，我们希望知道新的数据 xtest是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性$p(x)$。 上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。这种方法称为密度估计 if \quad p(x) \begin{cases} \leq\epsilon, & \text {anomaly} \\ >\epsilon, & \text{normal} \end{cases}高斯分布高斯分布也称为正态分布。变量$x$符合高斯分布$x~N(\mu,\sigma^2)$则其概率密度函数为: p(x,\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})可以利用已有数据来预测总体中的$\mu$和$\sigma^2$: \mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}\sigma^2 = \frac{1}{m}\sum\_{i=1}^{m}(x^{(i)}-\mu)^2算法对于给定数据集$x^{(1)},x^{(2)},x^{(3)}$，预测$\mu$和$\sigma^2$的值。给定一个训练实例，根据模型计算$p(x)$。 p(x) = \prod_{j=1}^np(x_j,u_j,\sigma_j^2)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-u_j)^2}{2\sigma_j^2})开发异常检测系统 根据训练集数据，估计特征的平均值和方差并构建$p(x)$函数，一般训练集数据都是正常数据。 对交叉检验集，尝试使用不同的$\epsilon$作于阈值，根据$F_1$或者查准率、查全率的比例来选择$\epsilon$。 选出$\epsilon$后，对测试集进行预测，计算异常检测系统的$F_1$或者查全率、查准率之比。 异常检测监督学习的比较 异常检测 监督学习 少量正向类，大量负向类 同时有大量正向和负向类 有许多不同种类的异常，未来遇到的异常可能与已掌握的异常非常不同根据非常少量的正向类数据来训练算法 有足够多的正向类实例，足够用于训练算法，未来遇到的正向类实例可能与训练集中的非常近似 欺诈行为检测、生产(例如飞机引擎检测)、检测数据中心计算机运行状况 邮件过滤、天气预报、肿瘤分类 选择特征异常检测特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能工作，但是最好将数据转换为高斯分布，常用对数函数$x=log(x+c)$、$x=x^c(c为0-1之间的一个分数)$。 误差分析：分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能从问题中发现需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小。 多元高斯分布假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。 在一般的高斯分布模型中，我们计算 的方法是： 通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 。 \mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}\Sigma=\frac{1}{m}(x^{(i)}-\mu)(x^{(i)}-\mu)^T=\frac{1}{m}(X-\mu)^T(X-\mu)p(x) = \frac{1}{(2\pi^{\frac{n}{2}})|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$|\Sigma|$是定矩阵，在Octave中用det(sigma)计算 协方差矩阵如何影响模型 上图是5个不同的模型，从左往右依次分析： 是一个一般的高斯分布模型； 通过协方差矩阵，令特征1拥有较小的偏差，同时保持特征2的偏差； 通过协方差矩阵，令特征2拥有较大的偏差，同时保持特征1的偏差； 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性； 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性 。 多元高斯分布与原高斯分布模型的比较 原高斯分布模型 多元高斯分布模型 不能捕捉特征之间的相关性，但可以通过将特征进行组合的方法来解决 自动捕捉特征之间的相关性 计算代价低，能适应大规模的特征 计算代价较高 训练集较小时也同样适用 - 必须满足 m&gt;n，不然协方差矩阵不可逆。通常须 m &gt; 10n。另外，特征冗余也会导致协方差矩阵不可逆 原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。 推荐系统基于内容的推荐 变量描述 $\theta^{(j)}$用户j的参数向量 $x^{(i)}$电影i的特征向量 对于用户j和电影i，我们预测评分为$(\theta^{(j)}x^{(i)})$ $n_u$用户的数量 $n_m$电影的数量 $r(i,j)$评分记录 $y(i,j)$用户i给电影j评分 $m_j$用户j评分过的电影总数 代价函数min\frac{1}{2}\sum\_{i:r(i,j)=1}((\theta^{(j)})^Tx^i-y^{(i,j)})^2 + \frac{\lambda}{2}(\theta_k^{(j)})^2其中 i :r(i,j) 表示我们只计算那些用户j评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以1/2m，在这里我们将m去掉。并且我们不对方差项 进行正则化处理。为了学习所有用户的参数，代价函数为： 梯度下降更新公式为： \theta_k^{(j)}:=\theta_k^{(j)}-\alpha\sum\_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} \quad k=0\theta_k^{(j)}:=\theta_k^{(j)}-\alpha(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} + \lambda\theta_k^{(j)} \quad k\neq0协同过滤在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。 但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。我们的优化目标便改为同时针对$x$和$\theta$进行： 对代价函数求偏导的结果如下： 初始 $x$,$\theta$为一些随机小值； 使用梯度下降算法最小化代价函数； 在训练完算法后，我们预测$(\theta^{(j)})^Tx^{(i)}$为用户 j 给电影 i 的评分。 如果一位用户正在观看电影，我们可以寻找另一部电影 ，依据两部电影的特征向量之间的距离$||x^{(i)}-x^{(j)}||$ 的大小来判断电影的相似度。 均值归一化如果已有数据如图 我们首先需要对结果 Y 矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值： 然后我们利用这个新的 Y，矩阵来训练算法。对于Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分。]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coursera Machine Learning ex6 附加题]]></title>
    <url>%2F2018%2F05%2F05%2FCoursera-Machine-Learning-ex6-%E9%99%84%E5%8A%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Machine Learning编程作业:用自己的数据训练垃圾邮件分类器Andrew Ng的machine learning公开课在支持向量机一章的附加题：自己构建数据集，使用SVM来训练垃圾邮件分类器,先放上实现的github链接。 数据预处理数据下载数据集链接 我使用了上面链接中的数据，其中名字中带hard是难以分辨是否为垃圾邮件的邮件，带ham的是正常邮件，带spam的是垃圾邮件。我把全部的垃圾邮件解压放到一个文件夹spam里，全部的正常邮件解压放到一个文件夹ham里。 邮件预处理运行getProcesseedEmail.m因为邮件中包含各种邮件头、网址、邮箱地址等信息，在对邮件进行词频统计和特征提取前需要先要把内容按照一定规范格式化一下，如去掉邮件头、全部改为小写、处理数字、网址、邮箱地址等。具体操作见代码中给出的myprocessEmail.m，处理结果放在./myDataset/processedSpam和./myDataset/processedHam文件夹中。 提取垃圾邮件高频词汇课程中的词汇表是直接给出的，可能不适用于下载的数据集，我们需要自己提取垃圾邮件高频词汇。 可是octave我不会玩啊(好像很会玩其他一样)，这个时候就要基于github编程了。 github启动！ 搜索“词频统计”！ 然后搜到了一个基于python NLTK的词频统计的实现。 clone! 好的，稍加修改我们就可以用了 我在统计时排除掉了一些常见的虚词，因为虚词无论在垃圾邮件中还是在正常邮件中，都挺高频的,这是稍加修改的实现 统计结果如图所示： 特征提取运行getFetaures.m统计结果有五万多条，后面大部分都是垃圾邮件为了混淆视听故意拼写错误的单词，我选取了前2000个单词作为特征。现在需要把每封邮件转化为一个特征向量。代码在作业中已经给出了。 123file_contents = readFile(&apos;emailSample1.txt&apos;);word_indices = processEmail(file_contents); % 得到单词索引features = emailFeatures(word_indices); % 得到特征向量 上面的代码是提取一封邮件的特征向量，下面的代码是提取所有邮件的特征向量(geneFeatures.m)。 1234567891011121314151617181920212223242526272829303132333435363738function [X,y] = geneFeatures(dataPath,spamPath,hamPath)spamPath = strcat(dataPath,spamPath); % 垃圾邮件目录hamPath = strcat(dataPath,hamPath); % 正常邮件目录spamList = dir(spamPath); hamList = dir(hamPath); spamFileNum = size(spamList,1); hamFileNum = size(hamList,1); spamX = zeros(spamFileNum,2001); % 所有垃圾邮件的特征向量，第2001列为标签hamX = zeros(hamFileNum,2001); % 所有正常邮件的特征向量，第2001列为标签for i = 3:spamFileNum, % 读取垃圾邮件目录下的所有垃圾邮件 fprintf(&apos;%d\n&apos;,i); filename = [spamPath spamList(i).name]; email_contents = readFile(filename); word_indices = processEmail(email_contents); features = emailFeatures(word_indices); spamX(i,:) = [features&apos; 1];end;for i = 3:hamFileNum, % 读取正常邮件目录下的所有正常邮件 fprintf(&apos;%d\n&apos;,i); filename = [hamPath hamList(i).name]; email_contents = readFile(filename); word_indices = processEmail(email_contents); features = emailFeatures(word_indices); hamX(i,:) = [features&apos; 0];end;X = [hamX;spamX]; % 合并两个特征矩阵rowrank = randperm(size(X, 1)); Xy = X(rowrank, :) % 打乱顺序X = Xy(:,1:2000); % 得到特征矩阵y = Xy(:,2001); % 得到标签save myTrainDataset X y; % 将X、y存储为文件myTrainDataset 经过上面的处理，我们就得到了我们训练需要的输入数据X、y,他们被存进了myTrainDataset文件，使用load命令就可以加载他们。(吐槽一下这个方法好慢，跑了一晚上，而且Octave很奇怪，代码里打印下进度，命令行里却不显示，不知道运行进度很考验耐心) 训练分类器运行MySpam.m本来打算使用LIBSVM库进行训练的，但是发现要自己编译，我没有安装Visual Studio，所以就使用作业中自带的训练方法来训练了(代码见MySpam.m)。这里可以使用自带的linearKernel核函数或者gaussianKernel核函数来训练。 12345678910111213load(&apos;TrainDataset&apos;);fprintf(&apos;\nTraining Linear SVM (Spam Classification)\n&apos;)fprintf(&apos;(this may take 1 to 2 minutes) ...\n&apos;)X = XTrain(1:4000,:); %提取4000条训练特征 y= yTrain(1:4000,:);XTest = XTrain(4001:,:); %剩下的都是验证集yTest = yTrain(4001:,:);C = 0.1;model = svmTrain(X, y, C, @linearKernel); % 训练p = svmPredict(model, XTest); %预测fprintf(&apos;Training Accuracy: %f\n&apos;, mean(double(p == yTest)) * 100); %验证 使用两个核函数进行训练的准确率如图。使用linearKernel，在训练集和测试集上都表现得不错 使用guassianKernel，在测试集上表现得不是很好，高方差有过拟合嫌疑，此时的$\sigma=1$ 调整$\sigma=10$，准确率如下,也都表现得不错。]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>practice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clustering]]></title>
    <url>%2F2018%2F05%2F01%2FClustering%2F</url>
    <content type="text"><![CDATA[聚类(Clustering)无监督学习(Unsupervised Learning)数据没有任何标签信息 聚类算法将一系列无标签的训练数据，输入到一个算法中，得到给定数据的内在结构。我们可能需要某种算法帮助我们寻找一种结构。 应用： 市场分割 社交网络分析 组织计算机集群 研究星系形成 K-均值算法(K-Means Algorithm)K-均值算法是普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的簇。 算法步骤: 选择k个随机的点作为聚类中心(cluster centroids) 将每个数据与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类 计算每一个组的平均值，将该组的中心点移动到平均值的位置 重复2-3步直至中心点不再变化 伪代码123456Repeat &#123; for i = 1 to m c(i) := index (form 1 to K) of cluster centroid closest to x(i) for k = 1 to K μk := average (mean) of points assigned to cluster k&#125; 优化目标(Optimization Objective)K-均值最小化问题，是要最小化所有数据与所关联的聚类中心点之间的距离之和，因此其代价函数(畸变函数)为: J(c^{(1)},...,c^{(m)},\mu^1,...,\mu^k)=\frac{1}{m}\sum_{i=1}^{m}||X^{(i)}-\mu_{c^{(i)}}||^2我们知道，第一个循环是用于减小 引起的代价，而第二个循环则是用于减小 引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。 随机初始化(Random Initialzation)步骤 选择$K\leq m$ 随机选择$K$个训练实例，然后令$K$个聚类中心分别与这$K$个训练实例相等 可能会停留在一个局部最小值，这取决于初始化的情况为了解决这个问题，我们通常需要多次运行K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。这种方法在k较小的时候还是可行的，但是如果k较大，这么做也可能不会有明显地改善。 选择K没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。 肘部法则当畸变值下降得慢的时候，说明此时的聚类个数较为合理 降维(Dimensionality Reduction)数据压缩(Motivation I_Data Compression)例如两种仪器对同一个东西测量的结果不完全相等，但是都特征却有些重复，这时候需要进行降维。 将数据从三维降至二维:将三维向量投射到一个二维平面上，强迫所有数据在同一平面上，降至二维的特征向量。 可视化(Motivation II_Visualization)只有三维以内的数据才方便可视化 问题降维算法只负责减少维数，新产生特征的意义需要自己发现 主成分分析(Principal Component Analysis)主成分分析是常见的降维算法，要做的是找到一个方向向量，把所有数据投射到该向量上时，我们希望投射平均均方误差尽可能地小。 主成分分析与线性回归的比较：主成分分析最小化的是投射误差，而线性回归尝试最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。 PCA技术的好处：对数据进行降维处理，对新求出的“主元”向量的重要性进行排序，根据需要选取最重要的部分。将后面的维数省去，可以达到降维从而简化模型或者对数据进行压缩。PCA无参数限制，不需要人为干预，但是如果对数据有先验知识，这却是一个缺点，因为无法通过参数化对处理过程进行干预。 主成分分析算法 均值归一化，计算出所有特征的均值，令x_j=x_j-\mu_j,如果特征在不同数量级上，还需要进行特征缩放，将其除以标准差$\sigma$ 计算协方差矩阵(covariance matrix)\sum=\frac{1}{m}(x^{(i)})(x^{(i)})^T,这是一个n*n的矩阵。 计算协方差矩阵$\sum$的特征向量(eigenvectors)奇异值分解: [U,S,V] = svd(sigma)对于一个 n×n维度的矩阵，上式中的U是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从n维降至k维，我们只需要从U中选取前k个向量，获得一个n×k维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量： z^{(i)} = U^T_{reduce} * x^{(i)} 选择主成分分析的数量主成分分析是减少投射的平均均方误差: \frac{1}{m}\sum_{i=1}^m||x^{(i)-x_{approx}^{(i)}}||^2训练集的方差为\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}||^2如果我们希望平均均方误差与训练集方差比例小于1%,这意味着原本数据偏差的99%都保留下来了。我们可以先令k=1，然后进行主要成分分析，获得 和z，然后计算比例是否小于 1%。如果不是再令k=2，如此类推，直到找到可以使得比例小于1%的最小k值（原因是各个特征之间通常情况存在某种相关性） 在svd函数中，S是一个n*n矩阵，只有对角线上有值，其他单元都是0，我们使用这个矩阵来计算平均均方误差与训练集方差的比例: 1-\frac{\sum_{i=1}^mS_{ii}}{\sum_{i=1}^{k}S_{ii}}\leq 1\%重建的压缩表示X_{approx} = U_{reduce}Z主成分分析法应用建议 如果我们有交叉验证集合测试集，也采用对训练集学习而来的$U_{reduce}$ 一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理 另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分。这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Support Vector Machines]]></title>
    <url>%2F2018%2F04%2F24%2FSupport-Vector-Machines%2F</url>
    <content type="text"><![CDATA[支持向量机(Support Vector Machines)优化目标监督学习算法的性能大多类似，因此重要的不是选择算法，而是应用算法的水平。诸如特征的选择以及正则化参数的选择。SVM在学习复杂的非线性方程时提供了更为清晰、强大的方式。 从逻辑回归来修改 在逻辑回归中，每个样本$(x,y)$都会为总代价函数增加一项。忽略$\frac{1}{m}$，当$y=1$,代价函数项为$-log(\frac{1}{1+e^{-\theta^Tx}})$，随着$Z$增大而减小，当$y=0$，代价函数项为$-log(1-\frac{1}{1+e^{-\theta^Tx}})$，随着$Z$增大而增大。在这个基础上支持向量机的代价函数，做了如图变换，即对于正样本的代价函数，$Z&gt;1$时代价函数为0，对于负样本的代价函数,$Z &lt; -1$时代价函数为0，其他时候为线性函数，暂时可以不必考虑其斜率如何。 左边的函数为$cost_1^{(z)}$,右边的函数为$cost_0^{(z)}$ 构建支持向量机有了代价函数形式，最小化问题为 \frac{1}{m}(\sum_{i=1}^{m}y^{(i)}(-logh\_\theta{(x^{(i)})})+(1-y^{(i)})(-log(1-h\_\theta(x^{(i)}))))然后再加上正则化参数由于$\frac{1}{m}$不影响优化，可以先不考虑。不同于逻辑回归中$A+\lambda B$的正则化优化形式，这里采用$CA+B$的正则化优化形式。所以，代价函数的形式为: min\_\Theta C\sum_{i=1}{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum\_{i=1}^n\theta_j^{2}假设函数的形式为: h\_\theta{(x)} =\begin{cases} 1, & \text{if $\theta^Tx\geq0$ } \\ 0, & \text{otherwise}\end{cases}与逻辑回归输出概率不同，SVM的假设函数直接预测y是1还是0 大边界的直观理解(Large Margin Intuition)支持向量机不仅要求正确分开输入样本，还引入了安全的间距因子，即$y=1$,我们希望$\theta^Tx \geq 1$,$y=0$，我们希望$\theta^Tx \leq -1$ 常数C的影响 若C非常大，我们希望在使第一项为0的前提下优化问题。要想让第一项为0，则要遵从以下约束，如果$y^{(i)}=1$,$\theta^Tx^{(i)} \geq 1$，如果$y^{(i)}=0$,$\theta^Tx^{(i)} \leq -1$。在这个基础上求解关于$\theta$的函数，会得到一个有趣的决策边界。 间距 上图可以有很多决策边界进行分割，但是支持向量机会选择黑色这条决策边界，它在分离正负样本上显得更好，就是这个黑线具有更大的**间距**。 **支持向量机的间距，是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本** 在代价函数最小化的过程中，我们希望找出$y=1$和$y=0$两种情况下都使得代价函数中左边一项尽量为0的参数。如果找到这样的参数，最小化问题变为。 $$min\frac{1}{2}\sum\_{j=1}^n\theta_j^2$$ 参数遵从的约束见“常数C的影响” 使用大间距分类器，学习算法会受**异常点**的影响。这样就需要考虑$C=\frac{1}{\lambda}$，回顾$\lambda$对过拟合和欠拟合的影响。 因此： $C$较大时，$\lambda$较小，可能会导致过拟合，高方差 $C$较小时，$\lambda$较大，可能会导致欠拟合，高偏差 ## 大边界分类的数学原理 ### 向量内积 $u^Tv$是向量$u$和$v$的内积，$||u||=\sqrt{u_1^2+u_2^2}$，再看向量$v$，已知$v_1$和$v_2$这两个分量，将向量$v$投影到$u$上，长度为$p$,那么另外一种计算内积的方法是$p||u||$(PS:我怎么觉得需要证明一下,想想原来是三角函数emmm)有了向量内积的知识，再来理解支持向量机中的目标函数。 决策边界需要满足的条件假如仅有两个特征,忽略截距$\theta_0$，那么$\theta=[\theta_1 \quad \theta_2]$，目标函数为$\frac{1}{2}||\theta||^2$,所以支持向量机所要做的就是极小化参数$\theta$长度的平方。 加入我们有很多决策边界都能把训练集分开，即满足上面讲的约束，那应该怎样选择参数$\theta$呢?事实上,参数$\theta$向量和决策边界是正交的(因为决策边界的斜率就和参数$\theta$的方向垂直) 所以要想尽量满足约束条件$p||\theta||\geq1$或者$p||\theta||\leq-1$，而且要让$||\theta||$尽量小，那么就要让$p$的绝对值尽量大，而如何让让$p$尽量大呢？就要让$x$与$\theta$之间的余弦值的绝对值越大，如何才能做到呢？支持向量机就会选择下边的决策边界，它试图极大化这些$p^{(i)}$的范数，即训练样本到决策边界的距离。 核函数(Kernels)给定一个训练实例$x$，我们利用$x$的各个特征与预先选定的地标(landmarks)$l^{(1)}$，$l^{(2)}$，$l^{(3)}$的近似程度来选取新的特征$f_1,f_2,f_3$。 例如: f_1 = similarity(x,l^{(l)})=exp(-\frac{||x-l^{(1)}||^2}{2\sigma^2})其中$||x-l^{(1)}||^2=\sum_{j=1}^n(x_j-l_j^{(1)})^2$，为实例$x$中所有特征与地标$l^{(1)}$之间的距离的核。具体而言,$similarity(x,l^{(1)})$就是核函数，高斯核函数(Gaussian Kernel)。如果训练实例与地标距离越短，则$f$近似于1，如果越长，则近似于0。$f$随$x$改变的速率受$\sigma$控制。 $f$是特征，所以$\theta^Tf$决定着假设函数预测的值 如何选择地标根据训练集数量选择地标数量，令$l^{(n)}=x^{(n)}$，新特征建立在原有特征与训练集中所有其他特征之间的距离的基础之上。 f^{(i)}=[f_0^{(i)}=1\quad... f_m^{(i)}=sim(x^{(i)},l^{(m)})]支持向量机假设 给定$x$，计算新特征$f$，当$\theta^T f\geq0$，预测$y=1$,反之 相应的代价函数为:minC\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum\_{j=1}^{n=m}\theta_j^{2} 其中$\sum_{j=1}^{n=m}\theta_j^{2}=\theta^T\theta$,需要用$\theta^T M\theta$代替,$M$是根据核函数不同选择的一个矩阵，这是为了简化计算。 没有介绍代价函数的计算方法，可以使用鲜有软件报进行计算(如liblinear,libsvm)。使用之前需要编写核函数，如果使用高斯核函数，使用之前进行特征缩放是必要的。 支持向量机也可以不适用核函数，或者成为线性核函数。 $C$和$\sigma$的影响$C=\frac{1}{\lambda}$ C 较大时，相当于λ较小，可能会导致过拟合，高方差； C 较小时，相当于λ较大，可能会导致低拟合，高偏差； $\sigma$较大时，可能会导致低方差，高偏差； $\sigma$较小时，可能会导致低偏差，高方差。 使用支持向量机高斯核函数之外的其他选择 多项式核函数（Polynomial Kernel） 字符串核函数（String kernel） 卡方核函数（ chi-square kernel） 直方图交集核函数（histogram intersection kernel） 这些核函数都要满足莫塞尔定理 多类分类一对多方法，k个类需要k个模型，k个参数向量$\theta$ 需要做的事 参数C的选择 选择核函数或相似函数 逻辑回归与支持向量机的使用准则n为特征数，m为训练样本数。 如果相较于m而言，n要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 如果n较小，而且m大小中等，例如n在 1-1000 之间，而m在10-10000 之间，使用高斯核函数的支持向量机。 如果n较小，而m较大，例如n在1-1000之间，而m大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。 当你有非常非常大的训练集，且用高斯核函数是在这种情况下，我经常会做的是尝试手动地创建，拥有更多的特征变量，然后用逻辑回归或者不带核函数的支持向量机 但是通常更加重要的是，你有多少数据，你有多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定你学习算法的变量等方面。通常这些方面会比你使用逻辑回归还是SVM这方面更加重要。]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[应用机器学习的建议]]></title>
    <url>%2F2018%2F04%2F15%2F%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[机器学习应用的建议《数学之美》这本书中有一句让我印象深刻的话：“许多失败并不是因为人不优秀，而是做事情的方法不对，一开始追求大而全的解决方案，之后长时间不能完成。最后不了了之”。在这一周的学习课程中，Andrew Ng同样给我们传达了这样的理念:”研究机器学习的东西，或者构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是构建一个简单的算法，这样你可以很快地实现它，然后再在这个基础上决定下一步怎么完善。 决定下一步 获得更多训练实例 减少特征数量 获得更多特征 增加多项式特征 减少正则化程度$\lambda$ 增加正则化程度$\lambda$ 评估假设(Evaluating a Hypothesis)过拟合问题将数据分为训练集和测试集。两种方法计算误差： 线性回归模型：计算代价函数$J$ 逻辑回归模型：计算代价函数$J$或者误分类比率err(h\_\theta(x),y)=\begin{cases}1 \quad if\quad h(x)\geq0.5 \quad and\quad y=0, of \quad if \quad h(x)]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Networks:Learning]]></title>
    <url>%2F2018%2F04%2F12%2FNerual-Networks-Learning%2F</url>
    <content type="text"><![CDATA[神经网络：学习(Neural Networks:Learning)代价函数标记方法 训练样本 m 输入 X 输出信号 y 神经网络层数 L 每层的神经元个数 $S_I$ 输出单元个数 $S_L$ 分类 二类分类 K类分类 公式逻辑回归中，只有一个输出，在神经网络中，有很多输出，$h_\theta{(x)}$是一个维度为K的向量 J(\Theta) = -\frac{1}{m} \sum_{i=1}^{m}\sum_{k=1}^{k}[ y_k^{(i)} log(( h\_\theta( x^{(i)}))\_k)+(1-y_k^{(i)})log(1-h(\_\theta(x^{(i)}))\_k)]+\frac{\lambda}{2m}\sum_{i=1}^{L-1}\sum_{i=1}^{S_l}\sum_{j=1}^{S^{l+1}}思想：观察算法预测的结果与真实的情况误差，与之前唯一不同的是，选取可能性最高的预测结果与实际数据比较。 正则化：排除了每一层$\theta_0​$后，每一层的$\theta​$矩阵的和 反向传播算法(Backpropagation Algorithm)定义首先计算最后一层的误差，然后一层一层反向求出各层的误差，直到第二层。 标记 网络层数 $l$ 计算层激活单元下标，下一层第j个输入变量下标 $j$ 受到权重矩阵第i行影响的下一层中误差单元下标 $i$ 误差 $\delta$ 误差矩阵(第$l$层的第i个激活单元受到第$j$个参数影响导致的误差) $\Delta_{ij}^{(l)}$示例 假设网络有四层 \delta^{(4)} = a^{(4)} - y\delta^{(3)} = ( \Theta^{(3)})^T \delta^{(4)} \cdot g'(z^{(3)})$g’(z^{(3)})$是$S$型函数的导数 g'(z^{(3)})=a^{(3)}\cdot (1-a^{(3)})\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}\cdot g'(z^{(2)})有了所有误差的表达式后，就可以计算代价函数的偏导数了。 不做正则化处理(求导公式待证明) \frac{\delta}{\delta\Theta\_{ij}^{(l)}}J{(\Theta)}=a_{ij}^{(l)}\delta_i^{l+1}算法表示结果: D\_{ij}^{(l)}:= \frac{1}{m} \Delta_{ij}^{(l)}+\lambda \Theta_{ij}^{(l)} \quad if \quad j \neq 0D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)} \quad if \quad j = 0权重在Octave的处理如果要使用fminnuc这样的优化算法来求解权重矩阵，需要将矩阵展开为向量，再利用算法来求出最优解后再转换为矩阵。 12345thetaVec = [Theta1(:) ; Theta2(:) ; Theta3(:)]...optimization using functions like fminuc... Theta1 = reshape(thetaVec(1:110), 10, 11); Theta2 = reshape(thetaVec(111:220), 10, 11); Theta1 = reshape(thetaVec(221:231), 1, 11); 直观理解(Intuition)如果考虑简单的没有正则化的二分类问题，代价函数为： cost(t)=y^{(t)}log( h\_\theta(x^{(i)}))+(1-y^{(t)})log(1-h\_\theta(x^{(t)}))直观地看，$\delta_j^{(l)}$是输入$a_j^{(l)}$的误差，正规的说，$\delta$就是代价函数的导数。 \delta_j^{(l)} = \frac{\delta}{\delta{z_j}^{(l)}}cost(t) 在这幅图中，要计算$\delta_2^{(2)}$, \delta_2^{(2)}=\Theta_{12}^{(2)}\delta_1^{(3)}+\Theta_{22}^{(2)}\delta_2^{(3)}其实这就是$\Theta^{(2)}$的第三列乘$\delta^{(3)}$ 梯度检验Numerical Gradient CheckinggradApprox = \frac{(J(\theta+\epsilon))-J(\theta-\epsilon))}{2\epsilon}当$\theta$是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对 进行检验的示例： \frac{\delta}{\delta\theta_1}=\frac{J(\theta_1+\epsilon_1,\theta_2,...,\theta_n)-J(\theta_1-\epsilon_1,\theta_2,...,\theta_n)}{2\epsilon}根据上面的算法，计算出的偏导数存储在矩阵$D_{ij}^{(l)}$ 中。检验时，我们要将该矩阵展开成为向量，同时我们也将$\theta$ 矩阵展开为向量，我们针对每一个$\theta$ 都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同 $D_{ij}^{(l)}$进行比较。 随机初始化如果全部参数一致，那么每个节点的值都是一样，这个神经网络每层实际只有一个单元。我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11 的参数矩阵，代码如下： 1Theta1 = rand(10, 11) * (2*eps) – eps 综合起来：训练步骤选择网络结构 第一层为训练集特征数量 最后一层为结果类数量 如果隐藏层大于一，确保每个隐藏层单元个数相同，通常隐藏层单元越多越好训练神经网络 参数随机初始化 正向传播算法计算所有$h_\theta(X)$ 计算代价函数J 反向传播算法计算所有偏导数 数值检验方法检验偏导数 使用优化算法优化代价函数 作业回顾一个三层的网络，输入层单元input_layer_size个，隐藏层单元hidden_layer_size个，分类结果num_labels个。X为m条训练数据，y是标签。 参数初始化123initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);initial_nn_params = [initial_Theta1(:) ; initial_Theta2(:)]; 正向传播算法1234567891011X = [ones(m, 1) X];ylabel = zeros(num_labels, m);for i=1:m ylabel(y(i), i) = 1;endz2 = X*Theta1&apos;;z2 = [ones(m, 1) z2];a2 = sigmoid(X*Theta1&apos;);a2 = [ones(m, 1) a2];a3 = sigmoid(a2*Theta2&apos;); 计算代价函数12345for i=1:m J = J - log(a3(i, :))*ylabel(:, i) - (log(1 - a3(i, :)) * (1 - ylabel(:, i)));endJ = J/m;J = J + lambda / (2*m) * ( sum(sum(Theta1(:,2:end).^2))+sum(sum(Theta2(:,2:end).^2)) ); 反向传播算法12345678910111213Delta1 = zeros(size(Theta1));Delta2 = zeros(size(Theta2));for t=1:m, delta_3 = a3(t,:)&apos; - ylabel(:,t); delta_2 = Theta2&apos;*delta_3.*(sigmoidGradient(z2(t,:)&apos;)); Delta1 = Delta1 + delta_2(2:end) * X(t, :); Delta2 = Delta2 + delta_3 * a2(t, :);endTheta1_grad = Delta1 / m;Theta1_grad(:, 2:end) = Theta1_grad(:, 2:end) + lambda/m*Theta1(:, 2:end);Theta2_grad = Delta2 / m;Theta2_grad(:, 2:end) = Theta2_grad(:, 2:end) + lambda/m*Theta2(:, 2:end); 数值检验123456789101112131415161718[cost, grad] = costFunc(nn_params);numgrad = computeNumericalGradient(costFunc, nn_params);function numgrad = computeNumericalGradient(J, theta)numgrad = zeros(size(theta));perturb = zeros(size(theta));e = 1e-4;for p = 1:numel(theta) % Set perturbation vector perturb(p) = e; loss1 = J(theta - perturb); loss2 = J(theta + perturb); % Compute Numerical Gradient numgrad(p) = (loss2 - loss1) / (2*e); perturb(p) = 0;endenddiff = norm(numgrad-grad)/norm(numgrad+grad); 优化算法123456[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1));Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ... num_labels, (hidden_layer_size + 1));]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Networks:Representation]]></title>
    <url>%2F2018%2F04%2F08%2FNeural-Networks-Representation%2F</url>
    <content type="text"><![CDATA[神经网络:表述(Neural Networks:Representation)神经元和大脑Ng试图阐述神经网络与人脑网络的联系(大雾)：大脑处理的方法可能是一个简单而完美的学习算法。人工智能的梦想就是：有一天能制造出真正的智能机器。而神经网络可能为我们打开一扇进入遥远的人工智能梦的窗户。 非线性假设线性回归与逻辑回归的缺点：当特征太多，计算负荷会非常大，假如有100个特征，两两组合也需会构成接近5000个新特征。若要处理一个有50x50像素的图片，将所有像素视为特征，两两组合会有接近300万个特征。普通逻辑回归模型不能有效处理。这时需要神经网络。 模型表示(Model Representation)神经元(neuron)又叫”激活单元”(activation unit):每一个神经元是一个学习模型，采纳一些特征作为输入，根据本身模型提供一个输出。 $x1$,$x2$,$x3$是输入单元(input units) $a_1^{(2)}$,$a_2^{(2)}$,$a_3^{(2)}$是中间单元，负责将数据进行处理，传递到下一层 最后是输出单元，负责计算$h_\theta^{(x)}$ 中间的箭头上存在参数，又称为权重(weight) 偏置单元$a_0^{(n)}$是一个单位向量 神经网络是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输出变量。 标记法 $a_i^{(j)}$代表第j层的第i个激活单元。 $\theta^{(j)}$代表从第$j$层映射到第$j+1$层时的权重的矩阵，其尺寸为$S_{j+1}*S_j+1$，S代表激活单元数量,这是因为偏置单元的原因。 上图所示模型(前向传播算法)，激活单元和输出分别表达为： 向量化这幅图值表达了将一个训练实例喂给神经网络的情况。如果输入整个训练集，则需要进行向量化： x = \begin{bmatrix} x_0 \\ x_1 \\ \vdots \\ x_n\\ \end{bmatrix} \theta = \begin{bmatrix} \theta_{10} & \cdots & \cdots & \cdots \\ \cdots & \cdots & \cdots & \cdots \\ \vdots & \vdots & \vdots & \ddots \\ \cdots & \cdots & \cdots & \theta_{m n} \\ \end{bmatrix} a = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_m\\ \end{bmatrix}\theta x=a从输入开始推导，令： z^{(2)}=\theta^{(1)} x,a^{(2)} = g(z^{(2)})计算后添加$a_0^{(2)}=1$。计算输出的值为 z^{(3)}=\theta a^{(2)}.h_\theta(x)=a^{(3)}=g(z^{(3)})现在考虑输入整个数据集: z^{(j+1)}=\theta^{(j)}X^T,a^{(j+1)} = g(z^{(j+1)})我们可以把$a$看成更为高级的特征值，也就是$x$ 的进化体，并且它们是由$x$与$\theta$ 决定的。因为是梯度下降的，所以$a$是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将$x$次方厉害，也能更好地预测新数据。这就是神经网络相比于逻辑回归和线性回归的优势。 直观理解逻辑运算AND $$X1,X2 \in {0,1}$$ $$y=x_1 \ AND \ x_2$$ $$\theta_0=-30,\theta_1=20,\theta_2=20$$ $$h_\theta(x)=g(-30+20x_1+20x_2)$$ 画出真值表和图像(略) $$h_\theta(x)\approx x_1\ AND \ x_2$$ ### 逻辑运算OR 原理同上，参数为： $$\theta_0=-10,\theta_1=20,\theta_2=20$$ ### 逻辑运算XNOR(同或) $$XNOR=(x_1 \ AND x_2) \ OR \ ((NOT \ x_1) \ AND \ (NOT \ x_2))$$ 首先构造一个$(NOT \ x_1) \ AND \ (NOT \ x_2)$的神经元: 参数为: $$\theta_0=10,\theta_1=-20,\theta_2=-20$$ 再将表示AND的神经元和表示$(NOT \ x_1) \ AND \ (NOT \ x_2)$的神经元通过$OR$进行组合： 按照这种方法我们可以构造越来越复杂的函数，得到更厉害的特征： 多类分类要分为多少个类，输出层就会有多少个假设函数，每一个输入在每一个假设函数的输出就代表了这个输入为此类的概率，在输出选取最大值作为分类结果。 作业回顾逻辑回归多类分类:lrCostFunction: 123456789function [J, grad] = lrCostFunction(theta, X, y, lambda)m = length(y); J = 0;grad = zeros(size(theta));J = 1/m * sum(-y.*(log(sigmoid(X*theta)))-(1-y).*log(1-sigmoid(X*theta))) + lambda/(2*m)*(sum(theta.*theta) - theta(1)*theta(1));grad = 1/m * (X&apos;*(sigmoid(X*theta)-y)) + lambda / m .* (theta);grad(1) = grad(1) - lambda / m .* (theta(1));grad = grad(:);end oneVsAll: 12345678910function [all_theta] = oneVsAll(X, y, num_labels, lambda)m = size(X, 1);n = size(X, 2);all_theta = zeros(num_labels, n + 1);X = [ones(m, 1) X];for c = 1:num_labels, options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 50); all_theta(c,:) = fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)),all_theta(c,:)&apos;, options);endend predictOneVsAll: 12345678function p = predictOneVsAll(all_theta, X)m = size(X, 1);num_labels = size(all_theta, 1);p = zeros(size(X, 1), 1);X = [ones(m, 1) X];p = sigmoid(X*all_theta&apos;);[ma,p] = max(p,[],2);end 神经网络多类分类$\theta^{(1)}$和$\theta^{(2)}已经训练完成$predict: 12345678910function p = predict(Theta1, Theta2, X)m = size(X, 1);num_labels = size(Theta2, 1);p = zeros(size(X, 1), 1);X = [ones(m, 1) X];p = sigmoid(X*Theta1&apos;);p = [ones(m,1) p];p = sigmoid(p*Theta2&apos;);[ma,p] = max(p,[],2);end]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2018%2F04%2F06%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[逻辑回归(Logistic regression)分类问题线性回归只能预测连续的值，分类问题只需要输出0或者1；当假设函数的值大于等于0.5，预测为1，小于0.5则预测为0。 回归模型假设(待证明)h_\theta(x)=g(\theta^Tx)g(z) = \frac{1}{1+e^{-z}} x表示特征向量 g代表逻辑函数 该函数的图像为 $h_\theta(x)$的作用是，对于给定的输入变量，根据参数计算输出变量为1 的可能性。 h_\theta(x)=P(y=1|x;\theta)判定边界(Decision Boundary)即$z=\theta^TX=0$ 代价函数若沿用线性回归的定义，得到的代价函数是一个非凸函数，这将会影响梯度下降算法寻找全局最小值 重新定义(待证明)J(\theta) = \frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)},y^{(i)})) Cost(h_\theta(x),y) = \begin{cases} -log(h_\theta(x)), & \text{if $y$=1} \\ -log(1-h_\theta(x)), & \text{if $y=0$} \end{cases}$h_\theta(x)$与$Cost(h_\theta(x),y)$之间的关系如图： 特点：$y=1，h_\theta(x) \neq1 $时误差随$h_\theta(x)变小而增大$，$y=0，h_\theta(x) \neq0 $时误差随$h_\theta(x)变大而增大$ 简化(待证明)基于最大似然估计 J(\theta)=-\frac{1}{m}\sum_{i=1}{m}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]梯度下降$\frac{\delta J(\theta)}{\delta\theta_j}=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}\frac{1}{h_\theta(x^{((i))})}\frac{\delta h_\theta(x^{(i)})}{\delta\theta_j}-(1-y^{(i)})\frac{1}{1-h_\theta(x^{(i)})}\frac{\delta h_\theta(x^{(i)})}{\delta\theta_j})$ $=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}\frac{1}{g(\theta^Tx^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^Tx^{(i)})})\frac{\delta g(\theta^Tx^{(i)}))}{\delta\theta_j}$ $=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}\frac{1}{g(\theta^Tx^{(i)})}-(1-y^{(i)})\frac{1}{1-g(\theta^Tx^{(i)})}) g(\theta^Tx^{(i)})(1-g(\theta^Tx^{(i)})x_j^{(i)})$ $=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}(1-g(\theta^Tx^{(i)})-(1-y^{(i)})g(\theta^Tx^{(i)}))x_j^{(i)} $ $=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}-g(\theta^Tx^{(i)}))x_j^{(i)}$ $=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}-y^{(i)}))x_j^{(i)}$ 第二步到第三步的推导： 可以证明(凸优化的知识)，所选代价函数是一个凸优化问题，没有局部最优值。注意，由于假设函数与线性回归不同，所以梯度下降是不一样的，同时特征缩放也是必要的。 一些梯度下降算法之外的选择 共轭梯度(Conjugate Gradient) 局部优化(BFGS) 有限内存局部优化(LBFGS) fminunc是octave和matlab提供的最小值优化函数，需要提供代价函数和每个参数的求导。 调用它的方式如下： options=optimset(‘GradObj’,’on’,’MaxIter’,100); initialTheta=zeros(2,1); [optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options); 简化代价函数和梯度下降J(\theta)=-\frac{1}{m}(y^Tlog(g(X\theta))+(1-y)^Tlog(1-g(X\theta)))grad = \frac{1}{m}.*(X^T(g(X\theta)-y))高级优化共轭梯度，局部优化，有限内存局部优化的优点：自动选择学习率$\alpha$,但是这些算法实际上在做更复杂的事情，不仅仅是在选择一个好的学习速率。 多类别分类(one-vs-all)若要将数据集分为n个类，则转化为n个二分类问题，做预测时将所有的分类机都运行一遍。 正则化(Regularization)过拟合(Overfitting)如果我们有非常多的特征，通过学习得到的假设可能能够非常好地适应训练集（代价函数可能几乎为0），但是可能会不能推广到新的数据。 如何处理过拟合 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙 正则化。保留所有的特征，但是减少参数的大小 正则化代价函数高次项导致了过拟合的产生，如果能让这些高此项系数接近于0，就能很好的拟合。 惩罚在尝试最小化代价时将惩罚纳入考虑 J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{n}\theta_j^{2}]$\lambda$称为正规化参数，根据惯例，不对$\theta_0$进行惩罚 $\lambda$过大，主要受$\theta_0$影响，欠拟合 正则化线性回归根据正则化代价函数求偏导\theta_j := \theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}正规方程也可以$\theta = (X^TX+\lambda \begin{bmatrix}0 &amp; &amp; &amp; &amp; \\ &amp; 1 &amp; &amp; &amp; \\&amp; &amp; &amp; \ddots &amp; \\&amp; &amp; &amp; &amp; 1 \\ \end{bmatrix})^-1X^Ty$]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 学习Shell]]></title>
    <url>%2F2018%2F04%2F02%2FLinux-%E5%AD%A6%E4%B9%A0Shell%2F</url>
    <content type="text"><![CDATA[学习shell什么是shellshell 就是一个程序，它接受从键盘输入的命令， 然后把命令传递给操作系统去执行 df 查看磁盘空间剩余数量 free 显示空闲内存数量 exit 结束终端会话 文件系统跳转文件命名规则 以”.”开头的文件名是隐藏文件，使用ls -a显示 文件名大小写敏感 没有文件扩展名概念 文件名可能包含空格标点，但是标点符号仅限使用”.”,”-“,”_”，最好不要使用空格 操作系统一些命令 less 浏览文件内容 ls 可以列出多个指定目录的内容 ls -a 全部 ls -d 目录 ls -F 在名字后面加上指示符 ls -h 以刻度格式列出 ls -l 长格式显示结果 ls -r 相顺序来显示结果 ls -S 按文件大小排序 ls -t 按修改时间来排序 确定文件类型 file filename less命令 Page UP or b 向上一页 Page Down or space 向下一页 UP Arrow 向上一行 Down Arrow 向下一行 G 移到最后一行 1G or g 移到开头一行 /characters 向前查找指定字符串 n 向前查找下一个出现的字符转 h 显示帮助屏幕 q 退出less程序 linux系统中的目录 / 根目录 /bin 包含系统启动和运行所需的二进制程序 /boot 包含Linux内核,初始RAM磁盘映像 /dev 包含所有设备节点 /etc 包含系统层面的配置文件 /home 用户目录 /lib 包含核心系统程序送所需共享库 /mnt 可移动介质挂载点 /opt 安装商业软件 /usr 包含普通用户所需的所有程序和文件 符号链接一个文件可能被子多个文件名所指向 操作文件和目录基本命令 cp mv mkdir rm ln 通配符 * 匹配任意多个字符（包括0个） ? 匹配任意一个字符（不包括0个） [characters] 匹配任意一个字符集中的字符 [!characters] 匹配任意一个不是字符集中的字符 [[:class:]] 匹配任意一个属于指定字符类的字符 [:alnum:] 匹配任意一个字符或数字 [:alpha:] 匹配任意一个字母 [:digit:] 匹配任意一个数字 [:lower:] 匹配任意一个小写字母 [:upper:] 匹配任意一个大写字母 cp复制文件和目录 -a 复制文件和目录以及属性 -i 在重写已存在的文件之前提示用户确认 -r 递归复制目录以及目录中的内容 -u 当把文件从一个目录复制到另一目录 -v 显示具体的操作信息 mv移动和重命名文件 -i 提示确认 -u 只移动不存在的文件夹 -v 显示具体的操作信息 rm删除文件和目录 -i 提示确认 -r 递归删除 -f 忽视不存在文件 -v 显示具体操作信息 小心rm命令 rm * .html 这会删除所有数据 ln创建链接 ln file link 符号链接 ln -s item link 硬链接 硬链接 每个文件默认有一个硬链接，硬链接给予文件名字。每创建一个硬链接，就为一个文件创建了一个额外的目录项。硬链接的局限：硬链接不能关联所在文件系统之外的文件，也不能关联一个目录。 符号链接类似于快捷方式，关联修改但是不关联删除。 使用命令基本命令 type 解释一个命令名 which 显示可执行程序的位置 man 显示命令手册页 apropos 显示一系列适合的命令 info 显示命令info whatis 显示一个命令的简洁描述 alias 建立命令别名 什么是命令 可执行程序 内建于shell自身的命令 一个shell函数 一个命令别名 重定向命令简介 cat 连接文件 sort 排序文本行 uniq 报道或省略重复行 grep 打印匹配行 we 打印文件中的换行符，字和字节个数 head 输出文件第一部分 tail 输出文件最后一部分 tee 从标准输入读取数据 标准输出 “&gt;” 将标准输出重定向到除屏幕以外的一个文件 “&gt;&gt;” 追加 标准错误shell内部使用0、1、2来重定向文件。0标准输入，1标准输出，2错误eg: ls -l /bin/usr 2 &gt; ls-error.txt 标准输出和错误到同一文件2&gt;&amp;1eg: &gt; ls-output.txt 2&gt;&amp;1 注意输出与错误的顺序eg: 2&gt;&amp;1 &gt; ls-output.txt 定向到屏幕 处理不需要的输出ls -l /bin/usr 2&gt; /dev/null 标准输入重定向eg：cat &gt; lazy_dog.txteg: cat &lt; lazy_dog.txt 管道线“|”一个命令的标准输出可以通过管道送至另一个命令的标准输入 过滤器过滤器接受输入，以某种方式改变它，然后输出它eg: ls /bin /usr/bin | sort | less uniq报道或忽略重复行常与sort命令结合eg: ls /bin /sur/bin | sort | uniq | less wc打印行数、字数、字节数grep打印匹配行eg: grep pattern [file…]eg: ls /bin /usr/bin | sort | uniq | grep zip方便选项:-i 使得执行搜索时忽略大小写-v 会告诉grep只打印不匹配的行 head / tail打印文件开头部分/结尾部分，用在管道中：ls /usr/bin | tail -n 5 tee从Stdin读取数据，并同时输出到Stdout和文件eg: ls /usr/bin | tee ls.txt | grep zip 从shell眼中看世界（字符）展开eg: echo * 将目录下文件名字展开 路径名展开eg: echo D or echo [[upper]] 隐藏文件路径名展开eg: echo . 会显示工作目录和父目录eg: ls -d .[!.]? 以圆点开头，第二个字符不包含圆点，再包含至少一个字符 算术表达式展开eg: echo $((expression))取幂：** 花括号展开eg: echo Front-{A,B,C}-BackFront-A-Back Front-B-Back Front-C-Backeg: echo Number_{1..5}Number_1 Number_2 Number_3 Number_4 Number_5eg: echo a{A{1,2},B{3,4}}baA1b aA2b aB3b aB4b那么这对什么有好处呢？最常见的应用是，创建一系列的文件或目录列表 参数展开eg: echo $USER 命令替换eg: echo $(ls) 双引号双引号可以阻止单词分割 单引号参数与表达式都不能生效，直接使用字符串 转义字符反斜杠 权限拥有者、组成员和其他人uid gid group 读取、写入、执行使用ls命令列出文件信息时，可以看到类似-rwxrwxrwx的字符串第一个字符表明文件类型： - 普通文件 d 一个目录 l 符号链接，符号链接的权限属性是虚拟的，真正的权限是所指向文件的属性 c 字符设备文件 b 块设备文件 后九个字符是权限属性： 前三个是所有者的权限 中间三个是所有者的组成员的权限 后面三个是所有人的权限 chmod改变文件模式八进制表示法符号表示法 进程查看进程 ps 查看进程 ps x 展示所有进程，不管由什么终端控制 aux “BSD风格”结果，可以查看进程USE，%CPU，%MEM，VSZ,RSS,START 进程状态 Stat R 运行中 S 正在睡眠 D 不可中断睡眠 T 已停止 Z 死进程 &lt; 高优先级进程 N 低优先级进程 top 动态查看进程进程放置到后台&amp; 进程前后台切换jobs 查看后台进程fg %job序号bg %job序号 停止一个进程kill pidkill [-signal] PID HUP 挂起 1 INT 中断（Crtl-c） 2 KILL 杀死 9 TERM 终止 15 CONT 继续 18 STOP 停止 19 默认为TERM killall给多个进程发送信号，杀死所有用户启动的进程 更多相关命令 pstree 树形结构进程 vmstat 输出系统资源使用快照，要看到连雪的结果，则在命令后加上更新操作延时的时间 tload 在终端中的xload程序]]></content>
      <tags>
        <tag>Linux 基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[误差平方和代价函数的来源]]></title>
    <url>%2F2018%2F04%2F01%2F%E8%AF%AF%E5%B7%AE%E5%B9%B3%E6%96%B9%E5%92%8C%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9A%84%E6%9D%A5%E6%BA%90%2F</url>
    <content type="text"><![CDATA[线性回归代价函数的来源Andrew Ng在Coursea的machine learning公开课上引入cost function：”And one thing I might do is try to minimize the square difference between the output of the hypothesis and the actual price of a house”那Cost Function是square difference的原因是什么呢？ 概率角度Andrew Ng在CS 229上有推导：数据集可以认为是从理想模型$F(x)$中采样，并添加高斯噪声形成。那数据集中的每一个点$(x^{(i)},y^{(i)})$都服从于均值为$f(x^{(i)})$，方差为某一固定值高斯分布。所以数据$(x^{(i)},y^{(i)})$的概率如下： p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt[2]{2\pi\sigma}}exp(\frac{(y^{(i)-\theta^Tx^{(i)}})^2}{2\sigma^2})要判断一个模型是否接近理想，可以比较数据集在当前模型下出现的概率，即最大似然估计。目标就是极大化数据集的对数似然函数。通过化简，极大化数据集的对数似然函数等价于最小化误差的平方和。 技术角度用梯度下降或者梯度上升来优化模型时，误差的平方和在求导时具有快速简洁的优势。 缺点 信号保真度与信号的空间和时间顺序无关 误差信号和原信号无关 信号的保真度和误差的符号无关]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear Regression Multiple Variables]]></title>
    <url>%2F2018%2F04%2F01%2FLinear-Regression-Multiple-Variables%2F</url>
    <content type="text"><![CDATA[多变量线性回归多变量线性回归的思想与单变量线性回归一致。要想利用数值计算工具方便快速地进行回归分析，则需要各个计算步骤向量化。 变量描述 $x_j^{(i)}$ 第$i$个训练数据中的第$j$个特征 $x^{(i)}$ 输入特征的$i$个训练数据 $m$ 训练数据的数量 $n$ 特征的数量 假设函数向量化h_\theta(x) = \begin{bmatrix} \theta_0&\theta_1&\cdots & \theta_n\end{bmatrix}\begin{bmatrix} x_0 \\ x_1 \\ \vdots \\ x_n \end{bmatrix} = \theta^Tx备注: $x_0^{(i)} = 1$ 多变量的梯度下降repeat until convergence: $\lbrace$$\quad \theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \quad for \quad j: = 0…n$$\rbrace$ 特征缩放当不同参数的尺度范围相差很远时，图像会很扁，需要很多次迭代才能收敛。如果将这些特征的特征都缩放到-1到1之间 或者 -0.5到0.5之间，就能较快地迭代 常用方法有特征缩放和均值归一化 x_i := \frac{(x_i - u_i)}{s_i}其中$u_i$是第$i$个特征的平均值，$s_i$是标准差或者最大值减最小值 学习率梯度下降算法的每次迭代受到学习率的影响，如果学习率α过小，则达到收敛所需的迭代次数会非常高；如果学习率α过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。 正规方程$\theta = (X^T)^{-1}X^Ty$ 比较：梯度下降需要选择学习率$\alpha$，需要多次迭代，当特征数量较多时能较好适用，适用于各种模型。正规方程不需要学习率，一次运算得出，通常n小于10000时还可以接受，只适用于线性模型，不适合逻辑回归等其他模型。 向量化(Vectorization)\theta = \theta - \frac{\alpha}{m}X'( X\theta-y)Octave教程数值计算 v = 1:1:6 v被赋值1至6的六个整数 ones(2,3) 生成全1阵 zeros(1,3) 生成全0阵 rand(3,3) 生成随机矩阵（介于0和1之间） eye(6) 生成单位矩阵 size(A) 返回矩阵大小，1x2矩阵 size(A,1) 矩阵行数 size(A,2) 矩阵列数 length(A) 最大维度 who 工作空间中的变量 clear 删除空间中的变量 save 文件名 变量名 将变量存为文件 A([1 3],:) 返回第1，3行的所有列 A（:,2） = [10;11;12] 第二列被替换 A = [A,[100;101;102]] 在矩阵右面附加列 A(:) 把所有元素放入单独列向量 C = [A,B] 把两个矩阵直接连在一起 A * C 向量相乘 A .* B 点积 A .^ 2 每个元素平方 1 ./ A 每个元素求倒数 log(A) 求对数 exp(A) 自然数e的幂次运算 A &lt; 3 返回小于3的元素位置，值为1 magics(n) n阶幻方 [r,c] = find(A &gt;=7 ) sum(a) 累加 prod(a) 累乘 floor(a) 向下四舍五入 ceil(a) 向上四舍五入 type(3) 3x3矩阵 max(A,[],1) 得到每一列最大值 max(A,[],2) 得到每一行最大值 sum(A,1) 每一列的总和 sum(A,2) 每一行的总和 flipup/flipud 向上/下翻转 pinv(A) 求伪逆矩阵绘图 plot(x,y) xlablel(‘’) 横轴名称 ylable(‘’) 纵轴名称 subplot(1,2,1) 将图像分为1*2格子，使用第一个格子 axis([0.5 1 -1 1]) 设置横轴和纵轴的范围 for,while,if语句 都需要end 与C语言类似 function [返回变量] = functionName(参数)]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Liner Regression]]></title>
    <url>%2F2018%2F03%2F31%2FLiner-Regression%2F</url>
    <content type="text"><![CDATA[引言什么是机器学习来自卡耐基梅隆大学的Tom Mitchell提出，一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅有了经验E后，经过评判，程序在处理T时的性能有所提升。 监督学习(Supervised Learning)给学习算法一个由“正确答案”组成的数据集，再根据这些样本做出预测，通常分为回归（连续的输出），分类（离散的结果）。 无监督学习(Unsupervised Learning)无监督学习中的数据集没有任何的标签，或者已有的标签都是相同的，其目的是找到数据中的结构，如聚类算法。 单变量线性回归(Linear Regression)模型表示(Model Representation)描述回归问题的标记 m 训练集中实例的数量 x 输入特征/输入变量 y 目标变量/输出变量 (x,y) 训练集中的实例 $(x^{(i)},x^{(i)})$ 第i个观察实例 h 学习算法的解决方案，假设(hypothesis)如何表达h对于单变量线性回归问题，一种可能的表达方式为：h_\theta(x)=\theta_0 + \theta_1x代价函数(Cost Function)建模误差(modeling error)：模型所预测的值与训练集中实际值之间的差距 代价函数直观理解（Intuition）最原始的假设函数是 h_\theta(x)=\theta_1x与之对应的代价函数是 J(\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2可以简化为求 J(\theta_1)=\frac{1}{2m}\sum_{i=1}^m(\theta_1x^{(i)}-y{(i)})^2的最小值此处$\theta_1$是才是变量，以一个初中生的视角来看，这个函数的图像是一个开口向上的二次函数，那么它必定存在最小值，如果假设函数加上$\theta_0$，$\theta_0$的变化会让原始代价函数(即开口向上的二次函数)图像在三维空间里发生变化，代价函数的图像会如图所示 梯度下降(Gradient Descent)梯度下降使用来求函数最小值的算法 思想随机选择一个参数的组合$(\theta_0,\theta_1,…,\theta_n)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。持续这么做直到得到一个局部最小值（local minimum）。 直观理解(Intuition)可以理解从山上以小碎步尽快下山，尽快下山的条件是每一个小碎步都是可迈出的步伐中陡峭的一步。怎么才算是最陡峭的呢？回到最原始的代价函数来看，这是一个二次函数，其实只有一种走法，就是按照让代价函数减小的方向去走,这个方向的变化可以定义为 \theta := \theta-\alpha\frac{\delta}{\delta\theta}J(\theta)这样定义的原因：当求导为负，代表随着$\theta$增大，代价函数在减小，在以上定义中$\theta$会变大，达到了目的，若求导为正亦然，另外$\alpha$是学习率，决定了下山每一步的大小。那如果有多个参数$\theta$呢？如何才是最陡峭的呢？可以将每个$\theta$理解为影响下山方向的因素，当每个$\theta$都按照让代价函数减小的方向去变化时，这时下山速度是最快的，即批量梯度下降。repeat until convergence$\lbrace$$\quad\theta_j = \theta_j - \alpha\frac{\delta}{\delta\theta_j}J(\theta_j)\quad$$\rbrace$ 关于梯度下降微妙的问题应该同时更新每个$\theta$,即 temp_0:=\theta_0 - \alpha \frac{\delta}{\delta\theta_0}J(\theta_0,\theta_1)temp_1:=\theta_1 - \alpha \frac{\delta}{\delta\theta_1}J(\theta_0,\theta_1)\theta_0 = temp_0\theta_1 = temp_1$\alpha$太小，迭代次数会很多，效率太低，$\alpha$太大，可能无法收敛，因为步子太大，会跨过最低点，此后就一直不断跨过最低点而无法收敛。 线性回归的梯度下降对代价函数求导\frac{\delta}{\delta\theta_0}J(\theta_0,\theta_1) = \frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\frac{\delta}{\delta\theta_1}J(\theta_0,\theta_1) = \frac{1}{m}\sum_{i=1}^mh_\theta((x^{(i)}-y^{(i)})x^{(i)})算法改写为：Repeat$\lbrace$ \theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) ​\theta_1 := \theta_1 - \alpha\frac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x^{(i)}))$\rbrace$ 线性代数回顾 加法和标量乘法 矩阵乘法不满足交换律，满足结合律 逆，转置]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>Coursera笔记</tag>
      </tags>
  </entry>
</search>
